"""
å¤‡ä»½æœåŠ¡ - é›†æˆR2äº‘å¤‡ä»½å’Œæœ¬åœ°å¤‡ä»½åŠŸèƒ½
"""

import asyncio
import logging
import os
import shutil
import subprocess
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

import boto3
from botocore.exceptions import ClientError
from dotenv import load_dotenv

logger = logging.getLogger(__name__)


class BackupService:
    """å¤‡ä»½æœåŠ¡"""

    def __init__(self):
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()
        
        self.backup_dir = Path("./backups")
        self.backup_dir.mkdir(exist_ok=True)

        # R2é…ç½®
        self.r2_config = {
            "access_key": os.getenv("R2_ACCESS_KEY_ID"),
            "secret_key": os.getenv("R2_SECRET_ACCESS_KEY"),
            "endpoint": os.getenv("R2_ENDPOINT"),
            "bucket": os.getenv("R2_BUCKET_NAME")
        }

        # å¤‡ä»½é…ç½®
        self.retention_days = int(os.getenv("BACKUP_RETENTION_DAYS", "30"))
        self.webhook_url = os.getenv("BACKUP_WEBHOOK_URL")

    async def create_backup(self, backup_type: str = "full", upload_to_r2: bool = True) -> str:
        """åˆ›å»ºå¤‡ä»½

        Args:
            backup_type: å¤‡ä»½ç±»å‹ (full, db_only, config_only)

        Returns:
            å¤‡ä»½æ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"flowslide_backup_{backup_type}_{timestamp}"
        backup_path = self.backup_dir / backup_name
        backup_path.mkdir(exist_ok=True)

        try:
            logger.info(f"ğŸ“¦ Creating {backup_type} backup: {backup_name}")

            if backup_type in ["full", "db_only", "data_only"]:
                await self._backup_database(backup_path)

            if backup_type in ["full", "config_only"]:
                await self._backup_config(backup_path)

            if backup_type in ["full", "media_only"]:
                await self._backup_uploads(backup_path)

            # Additional categorized content
            if backup_type in ["full", "templates_only"]:
                await self._backup_templates(backup_path)

            if backup_type in ["full", "reports_only"]:
                await self._backup_reports(backup_path)

            if backup_type in ["full", "scripts_only"]:
                await self._backup_scripts(backup_path)

            # å‹ç¼©å¤‡ä»½
            archive_path = await self._compress_backup(backup_path)

            # ä¸Šä¼ åˆ°R2ï¼ˆå¦‚æœé…ç½®äº†ä¸”æœªç¦ç”¨ï¼‰
            if upload_to_r2 and self._is_r2_configured():
                await self._upload_to_r2(archive_path)

            # æ¸…ç†æ—§å¤‡ä»½
            await self._cleanup_old_backups()

            # å‘é€é€šçŸ¥
            if self.webhook_url:
                await self._send_notification(backup_name, "success")

            logger.info(f"âœ… Backup completed: {archive_path}")
            return str(archive_path)

        except Exception as e:
            logger.error(f"âŒ Backup failed: {e}")
            if self.webhook_url:
                await self._send_notification(backup_name, "failed", str(e))
            raise

    async def create_external_sql_backup(self) -> str:
        """ä»…å¯¼å‡ºå¤–éƒ¨æ•°æ®åº“çš„ SQLï¼ˆä¸ä¸Šä¼ åˆ° R2ï¼‰ã€‚

        Returns:
            ç”Ÿæˆçš„ zip æ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"flowslide_external_sql_{timestamp}"
        backup_path = self.backup_dir / backup_name
        backup_path.mkdir(exist_ok=True)

        try:
            # strict=True: external SQL-only requires pg_dump to succeed
            await self._backup_external_database(backup_path, None, strict=True)
            archive_path = await self._compress_backup(backup_path)
            # ä¸ä¸Šä¼ åˆ° R2
            return str(archive_path)
        except Exception as e:
            logger.error(f"âŒ External SQL-only backup failed: {e}")
            raise

    async def _backup_database(self, backup_path: Path):
        """å¤‡ä»½æ•°æ®åº“"""
        try:
            # æ€»æ˜¯ä¼˜å…ˆå¤‡ä»½æœ¬åœ°SQLiteï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            db_file = Path("./data/flowslide.db")
            if db_file.exists():
                shutil.copy2(db_file, backup_path / "flowslide.db")
                logger.info("ğŸ’¾ Local SQLite database backup completed")

            # å¦‚é…ç½®äº†å¤–éƒ¨æ•°æ®åº“ï¼Œåˆ™é¢å¤–å¤‡ä»½å¤–éƒ¨æ•°æ®åº“
            try:
                from ..core.simple_config import EXTERNAL_DATABASE_URL
            except Exception:
                EXTERNAL_DATABASE_URL = os.getenv("DATABASE_URL", "")

            if EXTERNAL_DATABASE_URL and EXTERNAL_DATABASE_URL.startswith("postgres"):
                # strict=False: å¸¸è§„å¤‡ä»½ä¸­ç¼ºå°‘ pg_dump ä¸åº”å¯¼è‡´æ•´ä¸ªå¤‡ä»½å¤±è´¥
                await self._backup_external_database(backup_path, EXTERNAL_DATABASE_URL, strict=False)

        except Exception as e:
            logger.error(f"âŒ Database backup failed: {e}")
            raise

    async def _backup_external_database(self, backup_path: Path, external_url: Optional[str] = None, strict: bool = False):
        """å¤‡ä»½å¤–éƒ¨æ•°æ®åº“

        Behavior:
        - Prefer pg_dump for PostgreSQL.
        - If pg_dump is unavailable or fails and strict=True, fallback to Python-native COPY CSV export via psycopg2.
        - When strict=False, quietly skip external export on errors to not block other backup content.
        """
        try:
            # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥URLï¼Œå…¶æ¬¡ä»é…ç½®è·å–
            if not external_url:
                try:
                    from ..core.simple_config import EXTERNAL_DATABASE_URL as CFG_URL
                    external_url = CFG_URL
                except Exception:
                    external_url = os.getenv("DATABASE_URL", "")

            if not external_url:
                msg = "â„¹ï¸ No EXTERNAL_DATABASE_URL configured; skipping external DB backup"
                if strict:
                    raise RuntimeError("EXTERNAL_DATABASE_URL not configured for external SQL backup")
                logger.info(msg)
                return

            if not (external_url.startswith("postgresql://") or external_url.startswith("postgres://")):
                msg = "â„¹ï¸ External DB URL is not PostgreSQL; skipping pg_dump backup"
                if strict:
                    raise RuntimeError("External SQL backup only supports PostgreSQL URLs")
                logger.info(msg)
                return

            # å°è¯•å®šä½ pg_dump
            pg_dump_path = os.getenv("PG_DUMP_PATH")  # å¯æ˜¾å¼é…ç½®
            # å¦‚æœæ˜¾å¼è®¾ç½®äº†è·¯å¾„ä½†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™è§†ä¸ºæœªæ‰¾åˆ°
            if pg_dump_path and not os.path.isfile(pg_dump_path):
                pg_dump_path = None
            if not pg_dump_path:
                # ä½¿ç”¨ç³»ç»ŸPATHæŸ¥æ‰¾
                pg_dump_path = shutil.which("pg_dump")

            if not pg_dump_path:
                msg = (
                    "pg_dump not found. Please install PostgreSQL client tools and set PG_DUMP_PATH, or add pg_dump to PATH."
                )
                if strict:
                    logger.warning(f"{msg} Attempting Python-native COPY CSV fallback...")
                    try:
                        await self._export_external_postgres_copy(backup_path, external_url)
                        logger.info("âœ… External database exported via COPY CSV fallback")
                        return
                    except Exception as fallback_err:
                        raise RuntimeError(f"External export failed and no pg_dump: {fallback_err}")
                logger.warning(f"{msg} Skipping external SQL export for this backup.")
                return

            # è§£æURLï¼Œå°½é‡é¿å…å°†å¯†ç æš´éœ²åœ¨å‘½ä»¤è¡Œï¼ˆä½¿ç”¨ç¯å¢ƒå˜é‡PGPASSWORDï¼‰
            from urllib.parse import urlparse, parse_qs

            parsed = urlparse(external_url)
            username = parsed.username or "postgres"
            password = parsed.password or ""
            host = parsed.hostname or "localhost"
            port = str(parsed.port or 5432)
            dbname = parsed.path.lstrip("/") or "postgres"
            qs = parse_qs(parsed.query or "")
            sslmode = (qs.get("sslmode", [None])[0]) or ("require" if ("supabase" in (parsed.hostname or "") or "pooler.supabase.com" in external_url) else None)

            # Supabase æç¤ºï¼špg_dump éœ€è¦ç›´è¿æ•°æ®åº“å®ä¾‹ï¼Œä¸å»ºè®®ä½¿ç”¨ pooler ä¸»æœº
            if parsed.hostname and "pooler.supabase.com" in parsed.hostname:
                logger.warning("âš ï¸ Detected Supabase pooler host; pg_dump may fail. Prefer the direct db.<project>.supabase.co host for dumps.")

            # è¾“å‡ºæ–‡ä»¶
            out_file = backup_path / f"external_{dbname}.sql"

            # æ„å»ºå‘½ä»¤
            cmd = [
                pg_dump_path,
                "-h", host,
                "-p", port,
                "-U", username,
                "-d", dbname,
                "-F", "p",  # plain SQL
                "--no-owner",
                "--no-privileges",
                "-f", str(out_file),
            ]

            # ç¯å¢ƒå˜é‡ï¼šPGPASSWORD / PGSSLMODE
            env = os.environ.copy()
            if password:
                env["PGPASSWORD"] = password
            if sslmode:
                env["PGSSLMODE"] = sslmode

            logger.info(f"ğŸ›¸ Running pg_dump for external DB '{dbname}' on {host}:{port} -> {out_file.name}")

            def _run_dump():
                result = subprocess.run(cmd, env=env, capture_output=True, text=True)
                return result

            result = await asyncio.to_thread(_run_dump)

            if result.returncode != 0:
                stderr = (result.stderr or "").strip()
                stdout = (result.stdout or "").strip()
                if strict:
                    logger.warning(f"pg_dump failed (code {result.returncode}): {stderr or stdout}. Attempting COPY CSV fallback...")
                    try:
                        await self._export_external_postgres_copy(backup_path, external_url)
                        logger.info("âœ… External database exported via COPY CSV fallback")
                        return
                    except Exception as fallback_err:
                        raise RuntimeError(f"pg_dump failed and COPY fallback failed: {fallback_err}")
                logger.warning(f"pg_dump failed (code {result.returncode}), skipping external SQL export: {stderr or stdout}")
                return

            if not out_file.exists() or out_file.stat().st_size == 0:
                if strict:
                    logger.warning("pg_dump produced no output file; attempting COPY CSV fallback...")
                    try:
                        await self._export_external_postgres_copy(backup_path, external_url)
                        logger.info("âœ… External database exported via COPY CSV fallback")
                        return
                    except Exception as fallback_err:
                        raise RuntimeError(f"pg_dump produced no output and COPY fallback failed: {fallback_err}")
                logger.warning("pg_dump produced no output file; skipping external SQL export")
                return

            logger.info(f"âœ… External PostgreSQL dump completed: {out_file} ({out_file.stat().st_size} bytes)")

        except Exception as e:
            logger.error(f"âŒ External database backup failed: {e}")
            # æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨æ–¹å†³å®šæ˜¯å¦ç»§ç»­
            raise

    async def _export_external_postgres_copy(self, backup_path: Path, external_url: str) -> None:
        """ä½¿ç”¨ psycopg2 æ‰§è¡Œ PostgreSQL çš„ CSV å¯¼å‡ºï¼ˆæ•°æ®-onlyï¼Œschema ä¸åŒ…å«ï¼‰ã€‚

        ç”Ÿæˆæ–‡ä»¶ï¼š
        - tables/<schema>.<table>.csv æ¯ä¸ªè¡¨ä¸€ä»½ CSVï¼ˆUTF-8ï¼Œå¸¦è¡¨å¤´ï¼‰
        - external_copy_manifest.json å…ƒæ•°æ®ï¼ˆè¡¨æ¸…å•ã€å¯¼å‡ºæ—¶é—´ã€è¿æ¥ä¸»æœº/åº“åï¼‰
        """
        import json
        from urllib.parse import urlparse, parse_qs
        try:
            import psycopg2
        except Exception as ie:
            raise RuntimeError(f"psycopg2 not available: {ie}")

        parsed = urlparse(external_url)
        username = parsed.username or "postgres"
        password = parsed.password or ""
        host = parsed.hostname or "localhost"
        port = parsed.port or 5432
        dbname = parsed.path.lstrip("/") or "postgres"
        qs = parse_qs(parsed.query or "")
        sslmode = (qs.get("sslmode", [None])[0]) or None

        conn_kwargs = {
            "user": username,
            "password": password,
            "host": host,
            "port": port,
            "dbname": dbname,
        }
        if sslmode:
            conn_kwargs["sslmode"] = sslmode

        tables_dir = backup_path / "tables"
        tables_dir.mkdir(exist_ok=True)

        def _run_copy():
            with psycopg2.connect(**conn_kwargs) as conn:
                conn.autocommit = True
                with conn.cursor() as cur:
                    # è·å–ç”¨æˆ·è¡¨åˆ—è¡¨ï¼ˆæ’é™¤ç³»ç»Ÿ schemaï¼‰
                    cur.execute(
                        """
                        SELECT table_schema, table_name
                        FROM information_schema.tables
                        WHERE table_type='BASE TABLE'
                          AND table_schema NOT IN ('pg_catalog','information_schema')
                        ORDER BY table_schema, table_name
                        """
                    )
                    rows = cur.fetchall()
                    exported = []
                    for schema, table in rows:
                        safe_name = f"{schema}.{table}"
                        out_path = tables_dir / f"{schema}.{table}.csv"
                        sql = f"COPY \"{schema}\".\"{table}\" TO STDOUT WITH CSV HEADER"
                        with open(out_path, "w", encoding="utf-8", newline="") as f:
                            cur.copy_expert(sql, f)
                        exported.append({"schema": schema, "table": table, "file": out_path.name})

            # å†™å…¥æ¸…å•
            manifest = {
                "database": dbname,
                "host": host,
                "port": port,
                "exported_tables": exported,
                "exported_at": datetime.now().isoformat(),
                "format": "csv",
                "note": "Data-only export; schema DDL not included."
            }
            with open(backup_path / "external_copy_manifest.json", "w", encoding="utf-8") as mf:
                json.dump(manifest, mf, ensure_ascii=False, indent=2)

        # Run in thread to avoid blocking loop
        await asyncio.to_thread(_run_copy)

    async def _backup_config(self, backup_path: Path):
        """å¤‡ä»½é…ç½®æ–‡ä»¶"""
        config_files = [".env", "pyproject.toml", "uv.toml"]

        for config_file in config_files:
            if Path(config_file).exists():
                shutil.copy2(config_file, backup_path / config_file)

        logger.info("âš™ï¸ Config backup completed")

        # é™„åŠ ï¼šå¤åˆ¶ src/config ä¸‹çš„é…ç½®ï¼ˆè‹¥å­˜åœ¨ï¼‰
        cfg_dir = Path("./src/config")
        if cfg_dir.exists() and cfg_dir.is_dir():
            dest = backup_path / "src_config"
            shutil.copytree(cfg_dir, dest, dirs_exist_ok=True)
            logger.info("ğŸ“ src/config included in config backup")

    async def _backup_uploads(self, backup_path: Path):
        """å¤‡ä»½ä¸Šä¼ æ–‡ä»¶"""
        uploads_dir = Path("./uploads")
        if uploads_dir.exists():
            shutil.copytree(uploads_dir, backup_path / "uploads", dirs_exist_ok=True)
            logger.info("ğŸ“ Uploads backup completed")

    async def _backup_templates(self, backup_path: Path):
        """å¤‡ä»½æ¨¡æ¿ç¤ºä¾‹"""
        t_dir = Path("./template_examples")
        if t_dir.exists():
            shutil.copytree(t_dir, backup_path / "template_examples", dirs_exist_ok=True)
            logger.info("ğŸ“š Template examples backup completed")

    async def _backup_reports(self, backup_path: Path):
        """å¤‡ä»½ç ”ç©¶æŠ¥å‘Š"""
        r_dir = Path("./research_reports")
        if r_dir.exists():
            shutil.copytree(r_dir, backup_path / "research_reports", dirs_exist_ok=True)
            logger.info("ğŸ“‘ Research reports backup completed")

    async def _backup_scripts(self, backup_path: Path):
        """å¤‡ä»½è„šæœ¬"""
        s_dir = Path("./scripts")
        if s_dir.exists():
            shutil.copytree(s_dir, backup_path / "scripts", dirs_exist_ok=True)
            logger.info("ğŸ”§ Scripts backup completed")

    async def _compress_backup(self, backup_path: Path) -> Path:
        """å‹ç¼©å¤‡ä»½æ–‡ä»¶"""
        import zipfile

        archive_path = backup_path.with_suffix('.zip')

        with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_path in backup_path.rglob('*'):
                if file_path.is_file():
                    zipf.write(file_path, file_path.relative_to(backup_path.parent))

        # åˆ é™¤åŸå§‹å¤‡ä»½ç›®å½•
        shutil.rmtree(backup_path)

        logger.info(f"ğŸ—œï¸ Backup compressed: {archive_path}")
        return archive_path

    async def _upload_to_r2(self, backup_path: Path):
        """ä¸Šä¼ å¤‡ä»½åˆ°R2"""
        if not self._is_r2_configured():
            logger.info("â„¹ï¸ R2 not configured, skipping cloud backup")
            return

        try:
            logger.info(f"â˜ï¸ Starting R2 upload: {backup_path.name}")

            # åˆ›å»ºS3å®¢æˆ·ç«¯ï¼Œé…ç½®ä¸ºR2
            from botocore.config import Config
            config = Config(
                region_name='auto',
                retries={'max_attempts': 3, 'mode': 'standard'},
                read_timeout=30,
                connect_timeout=15,
                signature_version='s3v4'
            )

            s3_client = boto3.client(
                's3',
                aws_access_key_id=self.r2_config['access_key'],
                aws_secret_access_key=self.r2_config['secret_key'],
                endpoint_url=self.r2_config['endpoint'],
                config=config
            )

            # ç”ŸæˆæŒ‰ç±»åˆ«çš„å‰ç¼€ä¸æ—¥æœŸç›®å½•
            backup_date = datetime.now().strftime("%Y-%m-%d")
            # æœŸæœ›æ–‡ä»¶åæ ¼å¼ï¼šflowslide_backup_{type}_YYYYMMDD_HHMMSS.zip
            name = backup_path.name
            type_segment = "misc"
            try:
                # flowslide_backup_ + rest
                rest = name[len("flowslide_backup_"):]
                type_segment = rest.split("_")[0] or "misc"
            except Exception:
                type_segment = "misc"

            # å°† db_only å½’ç±»åˆ° database å‰ç¼€ï¼›å…¶ä»–èµ° categories/{type}
            if type_segment == "db_only":
                s3_key = f"backups/database/{backup_date}/{backup_path.name}"
            else:
                s3_key = f"backups/categories/{type_segment}/{backup_date}/{backup_path.name}"

            # ä¸Šä¼ æ–‡ä»¶
            logger.info(f"Uploading to R2: {self.r2_config['bucket']}/{s3_key}")

            # ä½¿ç”¨asyncio.to_threadåœ¨åå°çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥çš„boto3æ“ä½œ
            await asyncio.to_thread(
                s3_client.upload_file,
                str(backup_path),
                self.r2_config['bucket'],
                s3_key
            )

            logger.info(f"âœ… R2 upload completed successfully: {backup_path.name}")

        except ClientError as e:
            error_code = e.response['Error']['Code']
            error_msg = e.response['Error']['Message']
            logger.error(f"âŒ R2 upload failed (AWS Error {error_code}): {error_msg}")
            raise Exception(f"R2 upload failed: {error_msg}")
        except Exception as e:
            logger.error(f"âŒ R2 upload failed: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºæœ¬åœ°å¤‡ä»½å·²ç»æˆåŠŸ

    def _is_r2_configured(self) -> bool:
        """æ£€æŸ¥R2æ˜¯å¦é…ç½®"""
        return all(self.r2_config.values())

    async def _cleanup_old_backups(self):
        """æ¸…ç†æ—§å¤‡ä»½"""
        try:
            cutoff_date = datetime.now().timestamp() - (self.retention_days * 24 * 60 * 60)

            for backup_file in self.backup_dir.glob("*.zip"):
                if backup_file.stat().st_mtime < cutoff_date:
                    backup_file.unlink()
                    logger.info(f"ğŸ—‘ï¸ Cleaned up old backup: {backup_file.name}")

        except Exception as e:
            logger.error(f"âŒ Cleanup failed: {e}")

    async def _send_notification(self, backup_name: str, status: str, error: Optional[str] = None):
        """å‘é€å¤‡ä»½é€šçŸ¥"""
        if not self.webhook_url:
            return

        try:
            import aiohttp

            message = {
                "backup_name": backup_name,
                "status": status,
                "timestamp": datetime.now().isoformat(),
                "error": error
            }

            async with aiohttp.ClientSession() as session:
                await session.post(self.webhook_url, json=message)

        except Exception as e:
            logger.error(f"âŒ Notification failed: {e}")

    async def sync_to_r2(self, backup_path: Optional[Path] = None) -> Dict[str, Any]:
        """åŒæ­¥å¤‡ä»½åˆ°R2äº‘å­˜å‚¨

        Args:
            backup_path: æŒ‡å®šçš„å¤‡ä»½æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ€æ–°çš„å¤‡ä»½

        Returns:
            åŒæ­¥ç»“æœä¿¡æ¯
        """
        if not self._is_r2_configured():
            raise Exception("R2äº‘å­˜å‚¨æœªé…ç½®")

        try:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šå¤‡ä»½è·¯å¾„ï¼Œä½¿ç”¨æœ€æ–°çš„å¤‡ä»½
            if backup_path is None:
                backups = await list_backups()
                if not backups:
                    raise Exception("æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶è¿›è¡ŒåŒæ­¥")
                backup_path = Path(backups[0]["path"])

            # ç¡®ä¿å¤‡ä»½æ–‡ä»¶å­˜åœ¨
            if not backup_path.exists():
                raise FileNotFoundError(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_path}")

            logger.info(f"â˜ï¸ Starting R2 sync: {backup_path.name}")

            # ä¸Šä¼ åˆ°R2
            await self._upload_to_r2(backup_path)

            sync_info = {
                "filename": backup_path.name,
                "size": backup_path.stat().st_size,
                "timestamp": datetime.now().isoformat(),
                # created_at is the file mtime (when the backup file was created)
                "created_at": datetime.fromtimestamp(backup_path.stat().st_mtime).isoformat(),
                "success": True
            }

            logger.info(f"âœ… R2 sync completed: {backup_path.name}")
            return sync_info

        except Exception as e:
            logger.error(f"âŒ R2 sync failed: {e}")
            raise

    def list_backups(self) -> list:
        """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½"""
        backups = []
        for backup_file in self.backup_dir.glob("*.zip"):
            stat = backup_file.stat()
            backups.append({
                "name": backup_file.name,
                "size": stat.st_size,
                "created": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                "path": str(backup_file)
            })

        return sorted(backups, key=lambda x: x["created"], reverse=True)

    async def restore_backup(self, backup_name: str) -> bool:
        """æ¢å¤å¤‡ä»½"""
        backup_path = self.backup_dir / backup_name
        if not backup_path.exists():
            raise FileNotFoundError(f"Backup not found: {backup_name}")

        try:
            logger.info(f"ğŸ”„ Restoring backup: {backup_name}")

            # åˆ›å»ºä¸´æ—¶æ¢å¤ç›®å½•
            restore_temp_dir = self.backup_dir / f"restore_temp_{int(time.time())}"
            restore_temp_dir.mkdir(exist_ok=True)

            def extract_and_restore():
                import zipfile
                import shutil
                from pathlib import Path

                try:
                    # è§£å‹å¤‡ä»½æ–‡ä»¶
                    logger.info(f"ğŸ“¦ Extracting backup: {backup_name}")
                    with zipfile.ZipFile(str(backup_path), 'r') as zip_ref:
                        zip_ref.extractall(str(restore_temp_dir))

                    # æŸ¥æ‰¾æ•°æ®åº“æ–‡ä»¶ï¼ˆé€’å½’æŸ¥æ‰¾ï¼‰
                    db_files = list(restore_temp_dir.rglob("*.db"))
                    if not db_files:
                        raise Exception("å¤‡ä»½æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æ•°æ®åº“æ–‡ä»¶")

                    db_file = db_files[0]
                    logger.info(f"ğŸ—„ï¸ Found database file: {db_file.name} at {db_file}")

                    # å¤‡ä»½å½“å‰æ•°æ®åº“
                    current_db_path = Path("./data/flowslide.db")
                    if current_db_path.exists():
                        backup_current = current_db_path.with_suffix('.db.backup')
                        shutil.copy2(str(current_db_path), str(backup_current))
                        logger.info(f"ğŸ’¾ Backed up current database to: {backup_current}")

                    # æ¢å¤æ•°æ®åº“æ–‡ä»¶
                    shutil.copy2(str(db_file), str(current_db_path))
                    logger.info(f"âœ… Database restored from: {db_file.name}")

                    # æ¢å¤ä¸Šä¼ æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    uploads_dirs = list(restore_temp_dir.rglob("uploads"))
                    if uploads_dirs:
                        uploads_dir = uploads_dirs[0]
                        if uploads_dir.exists() and uploads_dir.is_dir():
                            target_uploads = Path("./uploads")
                            if target_uploads.exists():
                                shutil.rmtree(str(target_uploads))
                            shutil.copytree(str(uploads_dir), str(target_uploads))
                            logger.info("ğŸ“ Uploads directory restored")

                    # æ¢å¤é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    config_files = list(restore_temp_dir.rglob("*.json")) + list(restore_temp_dir.rglob("*.yaml")) + list(restore_temp_dir.rglob("*.yml"))
                    for config_file in config_files:
                        if "flowslide" in config_file.name.lower():
                            target_config = Path(".") / config_file.name
                            shutil.copy2(str(config_file), str(target_config))
                            logger.info(f"âš™ï¸ Config file restored: {config_file.name}")

                    logger.info("âœ… Backup restored successfully")
                    return True

                except Exception as e:
                    logger.error(f"âŒ Restore operation failed: {e}")
                    # å°è¯•æ¢å¤åŸå§‹æ•°æ®åº“
                    current_db_path = Path("./data/flowslide.db")
                    backup_current = current_db_path.with_suffix('.db.backup')
                    if backup_current.exists():
                        shutil.copy2(str(backup_current), str(current_db_path))
                        logger.info("ğŸ”„ Original database restored from backup")
                    raise
                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if restore_temp_dir.exists():
                        shutil.rmtree(str(restore_temp_dir))
                        logger.info("ğŸ§¹ Temporary restore files cleaned up")

            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œæ¢å¤æ“ä½œ
            await asyncio.to_thread(extract_and_restore)
            return True

        except Exception as e:
            logger.error(f"âŒ Restore failed: {e}")
            return False

    async def restore_from_r2(self) -> Dict[str, Any]:
        """ä»R2æ¢å¤æœ€æ–°çš„å¤‡ä»½ï¼Œå¦‚æœR2ä¸å¯ç”¨åˆ™ä½¿ç”¨æœ¬åœ°å¤‡ä»½"""
        try:
            logger.info("ğŸ”„ Starting R2 restore...")

            # é¦–å…ˆå°è¯•R2æ¢å¤
            try:
                return await self._restore_from_r2_cloud()
            except Exception as r2_error:
                logger.warning(f"âš ï¸ R2æ¢å¤å¤±è´¥: {r2_error}")
                logger.info("ğŸ”„ å°è¯•ä½¿ç”¨æœ¬åœ°å¤‡ä»½æ¢å¤...")

                # å›é€€åˆ°æœ¬åœ°å¤‡ä»½
                return await self._restore_from_local_backup()

        except Exception as e:
            logger.error(f"âŒ æ‰€æœ‰æ¢å¤æ–¹æ³•éƒ½å¤±è´¥: {e}")
            raise

    async def _restore_from_r2_cloud(self) -> Dict[str, Any]:
        """ä»R2äº‘å­˜å‚¨æ¢å¤"""
        # æ£€æŸ¥R2é…ç½®
        if not self._is_r2_configured():
            error_msg = "R2äº‘å­˜å‚¨æœªé…ç½®ã€‚è¯·æ£€æŸ¥ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼šR2_ACCESS_KEY_ID, R2_SECRET_ACCESS_KEY, R2_ENDPOINT, R2_BUCKET_NAME"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

        # éªŒè¯é…ç½®å€¼
        missing_configs = []
        if not self.r2_config.get('access_key'):
            missing_configs.append('R2_ACCESS_KEY_ID')
        if not self.r2_config.get('secret_key'):
            missing_configs.append('R2_SECRET_ACCESS_KEY')
        if not self.r2_config.get('endpoint'):
            missing_configs.append('R2_ENDPOINT')
        if not self.r2_config.get('bucket'):
            missing_configs.append('R2_BUCKET_NAME')

        if missing_configs:
            error_msg = f"R2é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘: {', '.join(missing_configs)}"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

        logger.info(f"âœ… R2é…ç½®éªŒè¯é€šè¿‡ - Bucket: {self.r2_config['bucket']}")

        # åˆ›å»ºS3å®¢æˆ·ç«¯ï¼Œé…ç½®ä¸ºR2
        try:
            from botocore.config import Config
            config = Config(
                region_name='auto',
                retries={'max_attempts': 3, 'mode': 'standard'},
                read_timeout=30,
                connect_timeout=15,
                signature_version='s3v4'
            )

            s3_client = boto3.client(
                's3',
                aws_access_key_id=self.r2_config['access_key'],
                aws_secret_access_key=self.r2_config['secret_key'],
                endpoint_url=self.r2_config['endpoint'],
                config=config
            )
            logger.info("âœ… S3å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            error_msg = f"åˆ›å»ºR2å®¢æˆ·ç«¯å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

        # åˆ—å‡ºR2ä¸­çš„å¤‡ä»½æ–‡ä»¶
        try:
            logger.info("ğŸ“‹ æ­£åœ¨åˆ—å‡ºR2ä¸­çš„å¤‡ä»½æ–‡ä»¶...")
            logger.info(f"ğŸ” æœç´¢å­˜å‚¨æ¡¶: {self.r2_config['bucket']}, å‰ç¼€: backups/")

            response = s3_client.list_objects_v2(
                Bucket=self.r2_config['bucket'],
                Prefix='backups/'
            )
            logger.info(f"âœ… æˆåŠŸè¿æ¥åˆ°R2å­˜å‚¨æ¡¶: {self.r2_config['bucket']}")
        except ClientError as e:
            error_code = e.response['Error']['Code']
            error_msg = e.response['Error']['Message']
            logger.error(f"âŒ æ— æ³•è®¿é—®R2å­˜å‚¨æ¡¶ (AWS Error {error_code}): {error_msg}")
            logger.error(f"ğŸ” è°ƒè¯•ä¿¡æ¯: å­˜å‚¨æ¡¶={self.r2_config['bucket']}, ç«¯ç‚¹={self.r2_config['endpoint']}")
            raise Exception(f"æ— æ³•è®¿é—®R2å­˜å‚¨æ¡¶: {error_msg}")
        except Exception as e:
            error_msg = f"è¿æ¥R2å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            logger.error(f"ğŸ” è°ƒè¯•ä¿¡æ¯: ç±»å‹={type(e).__name__}, æ¶ˆæ¯={str(e)}")
            raise Exception(error_msg)

        if 'Contents' not in response or not response['Contents']:
            error_msg = f"åœ¨R2å­˜å‚¨æ¡¶ '{self.r2_config['bucket']}' ä¸­æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶ã€‚è¯·ç¡®ä¿å·²ä¸Šä¼ å¤‡ä»½æ–‡ä»¶ã€‚"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

        # æ‰¾åˆ°æœ€æ–°çš„å¤‡ä»½æ–‡ä»¶
        latest_backup = max(response['Contents'], key=lambda x: x['LastModified'])
        backup_key = latest_backup['Key']
        backup_size = latest_backup['Size']
        backup_date = latest_backup['LastModified']

        logger.info(f"ğŸ“¥ æ‰¾åˆ°æœ€æ–°å¤‡ä»½: {backup_key}")
        logger.info(f"   ğŸ“… ä¿®æ”¹æ—¶é—´: {backup_date}")
        logger.info(f"   ğŸ“ æ–‡ä»¶å¤§å°: {backup_size} bytes")

        # ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
        try:
            self.backup_dir.mkdir(exist_ok=True)
            logger.info(f"âœ… å¤‡ä»½ç›®å½•å‡†å¤‡å°±ç»ª: {self.backup_dir}")
        except Exception as e:
            error_msg = f"åˆ›å»ºå¤‡ä»½ç›®å½•å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

        local_backup_path = self.backup_dir / Path(backup_key).name

        # ä¸‹è½½å¤‡ä»½æ–‡ä»¶
        try:
            logger.info(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½å¤‡ä»½æ–‡ä»¶åˆ°: {local_backup_path}")
            await asyncio.to_thread(
                s3_client.download_file,
                self.r2_config['bucket'],
                backup_key,
                str(local_backup_path)
            )
            logger.info("âœ… æ–‡ä»¶ä¸‹è½½å®Œæˆ")
        except ClientError as e:
            error_code = e.response['Error']['Code']
            error_msg = e.response['Error']['Message']
            logger.error(f"âŒ ä¸‹è½½å¤±è´¥ (AWS Error {error_code}): {error_msg}")
            raise Exception(f"ä»R2ä¸‹è½½å¤‡ä»½å¤±è´¥: {error_msg}")
        except Exception as e:
            error_msg = f"ä¸‹è½½å¤‡ä»½æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

        # éªŒè¯ä¸‹è½½çš„æ–‡ä»¶
        if not local_backup_path.exists():
            error_msg = f"ä¸‹è½½å®Œæˆåæ–‡ä»¶ä¸å­˜åœ¨: {local_backup_path}"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

        downloaded_size = local_backup_path.stat().st_size
        if downloaded_size != backup_size:
            error_msg = f"æ–‡ä»¶ä¸‹è½½ä¸å®Œæ•´ã€‚æœŸæœ›: {backup_size} bytes, å®é™…: {downloaded_size} bytes"
            logger.error(f"âŒ {error_msg}")
            # åˆ é™¤ä¸å®Œæ•´çš„æ–‡ä»¶
            local_backup_path.unlink(missing_ok=True)
            raise Exception(error_msg)

        logger.info(f"âœ… å¤‡ä»½æ–‡ä»¶éªŒè¯é€šè¿‡: {local_backup_path} ({downloaded_size} bytes)")

        # æ¢å¤å¤‡ä»½
        try:
            logger.info("ğŸ”„ æ­£åœ¨æ¢å¤å¤‡ä»½...")
            success = await self.restore_backup(local_backup_path.name)

            if success:
                restore_info = {
                    "filename": local_backup_path.name,
                    "size": downloaded_size,
                    "timestamp": datetime.now().isoformat(),
                    "source": "r2",
                    "r2_key": backup_key,
                    "r2_bucket": self.r2_config['bucket'],
                    "backup_date": backup_date.isoformat(),
                    "success": True
                }
                logger.info(f"âœ… R2æ¢å¤å®Œæˆ: {local_backup_path.name}")
                return restore_info
            else:
                error_msg = "å¤‡ä»½æ¢å¤è¿‡ç¨‹è¿”å›å¤±è´¥çŠ¶æ€"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)

        except Exception as e:
            error_msg = f"æ¢å¤å¤‡ä»½æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

    async def _restore_from_local_backup(self) -> Dict[str, Any]:
        """ä»æœ¬åœ°å¤‡ä»½æ¢å¤"""
        try:
            logger.info("ğŸ”„ ä½¿ç”¨æœ¬åœ°å¤‡ä»½è¿›è¡Œæ¢å¤...")

            # æŸ¥æ‰¾å¯ç”¨çš„æœ¬åœ°å¤‡ä»½
            if not self.backup_dir.exists():
                raise Exception("å¤‡ä»½ç›®å½•ä¸å­˜åœ¨")

            backup_files = list(self.backup_dir.glob("*.zip"))
            if not backup_files:
                raise Exception("æ²¡æœ‰æ‰¾åˆ°æœ¬åœ°å¤‡ä»½æ–‡ä»¶")

            # ä½¿ç”¨æœ€æ–°çš„å¤‡ä»½
            latest_backup = max(backup_files, key=lambda x: x.stat().st_mtime)
            logger.info(f"ğŸ“ ä½¿ç”¨æœ¬åœ°å¤‡ä»½: {latest_backup.name}")

            # æ¢å¤å¤‡ä»½
            success = await self.restore_backup(latest_backup.name)

            if success:
                restore_info = {
                    "filename": latest_backup.name,
                    "size": latest_backup.stat().st_size,
                    "timestamp": datetime.now().isoformat(),
                    "source": "local",
                    "success": True
                }
                logger.info(f"âœ… æœ¬åœ°å¤‡ä»½æ¢å¤å®Œæˆ: {latest_backup.name}")
                return restore_info
            else:
                raise Exception("æœ¬åœ°å¤‡ä»½æ¢å¤å¤±è´¥")

        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°å¤‡ä»½æ¢å¤å¤±è´¥: {e}")
            raise


# åˆ›å»ºå…¨å±€å¤‡ä»½æœåŠ¡å®ä¾‹
backup_service = BackupService()


async def create_backup(backup_type: str = "full", upload_to_r2: bool = True) -> str:
    """åˆ›å»ºå¤‡ä»½"""
    return await backup_service.create_backup(backup_type, upload_to_r2)


async def list_backups() -> list:
    """åˆ—å‡ºå¤‡ä»½"""
    return backup_service.list_backups()

async def create_external_sql_backup() -> str:
    """Module-level helper: ä»…å¯¼å‡ºå¤–éƒ¨æ•°æ®åº“ SQL åˆ°æœ¬åœ°å¤‡ä»½ã€‚"""
    return await backup_service.create_external_sql_backup()


async def list_r2_files(prefix: Optional[str] = None) -> list:
    """Module-level helper: åˆ—å‡ºR2ä¸­æŒ‡å®šå‰ç¼€ä¸‹çš„å¯¹è±¡ï¼ˆè¿”å› key/name, size, last_modifiedï¼‰

    Args:
        prefix: ä»…åˆ—å‡ºè¯¥å‰ç¼€ä¸‹çš„å¯¹è±¡ï¼›é»˜è®¤åˆ—å‡º backups/
    """
    # use global backup_service instance
    if not backup_service._is_r2_configured():
        return []

    try:
        from botocore.config import Config
        config = Config(region_name='auto')
        s3_client = boto3.client(
            's3',
            aws_access_key_id=backup_service.r2_config['access_key'],
            aws_secret_access_key=backup_service.r2_config['secret_key'],
            endpoint_url=backup_service.r2_config['endpoint'],
            config=config
        )
        effective_prefix = prefix or 'backups/'
        response = s3_client.list_objects_v2(
            Bucket=backup_service.r2_config['bucket'],
            Prefix=effective_prefix
        )
        items = []
        if 'Contents' in response and response['Contents']:
            for obj in response['Contents']:
                items.append({
                    'key': obj.get('Key'),
                    'size': obj.get('Size'),
                    'last_modified': obj.get('LastModified').isoformat() if obj.get('LastModified') else None
                })
        items.sort(key=lambda x: x.get('last_modified') or '', reverse=True)
        return items
    except Exception as e:
        logger.warning(f"list_r2_files failed: {e}")
        return []


async def delete_r2_file(key: str) -> bool:
    """Module-level helper: ä»R2åˆ é™¤æŒ‡å®šå¯¹è±¡ï¼ˆkeyï¼‰"""
    if not backup_service._is_r2_configured():
        raise Exception("R2æœªé…ç½®")

    try:
        from botocore.config import Config
        config = Config(region_name='auto')
        s3_client = boto3.client(
            's3',
            aws_access_key_id=backup_service.r2_config['access_key'],
            aws_secret_access_key=backup_service.r2_config['secret_key'],
            endpoint_url=backup_service.r2_config['endpoint'],
            config=config
        )

        await asyncio.to_thread(s3_client.delete_object, Bucket=backup_service.r2_config['bucket'], Key=key)
        logger.info(f"âœ… R2 object deleted: {key}")
        return True
    except Exception as e:
        logger.error(f"âŒ delete_r2_file failed: {e}")
        return False


async def restore_r2_key(key: str) -> Dict[str, Any]:
    """Module-level helper: ä»R2ä¸‹è½½æŒ‡å®š key å¹¶æ¢å¤è¯¥å¤‡ä»½ï¼ˆå°†æ–‡ä»¶ä¸‹è½½åˆ° backup_dir å¹¶è°ƒç”¨ restore_backupï¼‰"""
    if not backup_service._is_r2_configured():
        raise Exception("R2æœªé…ç½®")

    try:
        from botocore.config import Config
        config = Config(region_name='auto')
        s3_client = boto3.client(
            's3',
            aws_access_key_id=backup_service.r2_config['access_key'],
            aws_secret_access_key=backup_service.r2_config['secret_key'],
            endpoint_url=backup_service.r2_config['endpoint'],
            config=config
        )

        backup_service.backup_dir.mkdir(exist_ok=True)
        local_backup_path = backup_service.backup_dir / Path(key).name

        await asyncio.to_thread(
            s3_client.download_file,
            backup_service.r2_config['bucket'],
            key,
            str(local_backup_path)
        )

        if not local_backup_path.exists():
            raise Exception("ä¸‹è½½åçš„æ–‡ä»¶ä¸å­˜åœ¨")

        success = await backup_service.restore_backup(local_backup_path.name)
        if not success:
            raise Exception("æ¢å¤å¤±è´¥")

        return {
            'filename': local_backup_path.name,
            'size': local_backup_path.stat().st_size,
            'timestamp': datetime.now().isoformat(),
            'source': 'r2',
            'r2_key': key,
            'r2_bucket': backup_service.r2_config['bucket'],
            'success': True
        }
    except Exception as e:
        logger.error(f"âŒ restore_r2_key failed: {e}")
        raise


async def restore_backup(backup_name: str) -> bool:
    """æ¢å¤å¤‡ä»½"""
    return await backup_service.restore_backup(backup_name)
