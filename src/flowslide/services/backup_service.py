"""
å¤‡ä»½æœåŠ¡ - é›†æˆR2äº‘å¤‡ä»½å’Œæœ¬åœ°å¤‡ä»½åŠŸèƒ½
"""

import asyncio
import logging
import os
import shutil
import subprocess
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

import boto3
from botocore.exceptions import ClientError
from dotenv import load_dotenv

logger = logging.getLogger(__name__)


class BackupService:
    """å¤‡ä»½æœåŠ¡"""

    def __init__(self):
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()
        
        self.backup_dir = Path("./backups")
        self.backup_dir.mkdir(exist_ok=True)

        # R2é…ç½®
        self.r2_config = {
            "access_key": os.getenv("R2_ACCESS_KEY_ID"),
            "secret_key": os.getenv("R2_SECRET_ACCESS_KEY"),
            "endpoint": os.getenv("R2_ENDPOINT"),
            "bucket": os.getenv("R2_BUCKET_NAME")
        }

        # å¤‡ä»½é…ç½®
        self.retention_days = int(os.getenv("BACKUP_RETENTION_DAYS", "30"))
        self.webhook_url = os.getenv("BACKUP_WEBHOOK_URL")

    def refresh_r2_config(self) -> None:
        """Refresh R2 configuration from current environment (.env + process).

        Ensures runtime changes made via the UI are recognized without restart.
        """
        try:
            load_dotenv(override=True)
        except Exception:
            # best-effort reload
            pass
        self.r2_config.update({
            "access_key": os.getenv("R2_ACCESS_KEY_ID"),
            "secret_key": os.getenv("R2_SECRET_ACCESS_KEY"),
            "endpoint": os.getenv("R2_ENDPOINT"),
            "bucket": os.getenv("R2_BUCKET_NAME"),
        })

    async def create_backup(self, backup_type: str = "full", upload_to_r2: bool = True) -> str:
        """åˆ›å»ºå¤‡ä»½

        Args:
            backup_type: å¤‡ä»½ç±»å‹ (full, db_only, config_only, media_only, templates_only, reports_only, scripts_only, light)

        Returns:
            å¤‡ä»½æ–‡ä»¶è·¯å¾„
        """
        # æ‰©å±•æ”¯æŒè½»é‡æ•°æ®åŒæ­¥å¤‡ä»½ï¼šä»…åŒ…å« users / projects / ppt_templates / global_master_templates ä»¥åŠé…ç½®æ–‡ä»¶ JSON å¿«ç…§
        # è¯¥è½»é‡åŒ…ä¸å¤åˆ¶æ•°æ®åº“æ–‡ä»¶å’Œuploadsï¼Œä¾¿äºå¿«é€Ÿåœ¨å¦ä¸€å®ä¾‹è¿›è¡Œéƒ¨åˆ†åˆå¹¶æ¢å¤
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"flowslide_backup_{backup_type}_{timestamp}"
        backup_path = self.backup_dir / backup_name
        backup_path.mkdir(exist_ok=True)

        try:
            logger.info(f"ğŸ“¦ Creating {backup_type} backup: {backup_name}")

            if backup_type in ["full", "db_only", "data_only"]:
                await self._backup_database(backup_path)

            if backup_type in ["full", "config_only"]:
                await self._backup_config(backup_path)

            if backup_type in ["full", "media_only"]:
                await self._backup_uploads(backup_path)

            # Additional categorized content
            if backup_type in ["full", "templates_only"]:
                await self._backup_templates(backup_path)

            if backup_type in ["full", "reports_only"]:
                await self._backup_reports(backup_path)

            if backup_type in ["full", "scripts_only"]:
                await self._backup_scripts(backup_path)

            # ç‰¹æ®Šï¼šlight èµ°å®šåˆ¶JSONæ‰“åŒ…é€»è¾‘ï¼ˆå¿½ç•¥ä¸Šé¢å¯èƒ½åˆ›å»ºçš„ç©ºç›®å½•å†…å®¹ï¼‰
            if backup_type == "light":
                archive_path = await self._create_light_backup_archive(backup_path)
            else:
                # å‹ç¼©å¤‡ä»½
                archive_path = await self._compress_backup(backup_path)

            # ä¸Šä¼ åˆ°R2ï¼ˆå¦‚æœé…ç½®äº†ä¸”æœªç¦ç”¨ï¼‰
            if upload_to_r2 and self._is_r2_configured():
                await self._upload_to_r2(archive_path)

            # æ¸…ç†æ—§å¤‡ä»½
            await self._cleanup_old_backups()

            # å‘é€é€šçŸ¥
            if self.webhook_url:
                await self._send_notification(backup_name, "success")

            logger.info(f"âœ… Backup completed: {archive_path}")
            return str(archive_path)

        except Exception as e:
            logger.error(f"âŒ Backup failed: {e}")
            if self.webhook_url:
                await self._send_notification(backup_name, "failed", str(e))
            raise

    async def create_light_ephemeral_archive(self) -> Path:
        """åˆ›å»ºä¸è½åœ°(backupsç›®å½•)çš„è½»é‡å¤‡ä»½å‹ç¼©åŒ…, ä»…è¿”å›ä¸´æ—¶ zip è·¯å¾„ã€‚

        ç”¨äºâ€œåŒæ­¥åˆ°R2â€æŒ‰é’®ï¼šç”Ÿæˆåç›´æ¥ä¸Šä¼ å¹¶åˆ é™¤ï¼Œä¸è®¡å…¥æœ¬åœ°å¤‡ä»½åˆ—è¡¨ã€‚
        """
        from tempfile import TemporaryDirectory
        import json, sqlite3, zipfile
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        archive_name = f"flowslide_backup_light_{ts}.zip"
        tmp_dir_ctx = TemporaryDirectory()
        base = Path(tmp_dir_ctx.name) / "light_build"
        data_dir = base / "data"
        data_dir.mkdir(parents=True, exist_ok=True)

        db_file = Path("./data/flowslide.db")
        if db_file.exists():
            try:
                conn = sqlite3.connect(str(db_file))
                conn.row_factory = sqlite3.Row
                cur = conn.cursor()
                def dump(q, name):
                    try:
                        cur.execute(q)
                        rows = [dict(r) for r in cur.fetchall()]
                        (data_dir / name).write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding='utf-8')
                    except Exception as ie:
                        logger.warning(f"light ephemeral dump {name} failed: {ie}")
                dump("SELECT id, username, email, is_active, is_admin, created_at, updated_at, last_login, password_hash FROM users", "users.json")
                dump("SELECT id, project_id, title, scenario, topic, requirements, status, owner_id, outline, slides_html, slides_data, confirmed_requirements, project_metadata, version, created_at, updated_at FROM projects", "projects.json")
                dump("SELECT id, project_id, template_type, template_name, description, html_template, applicable_scenarios, style_config, usage_count, created_at, updated_at FROM ppt_templates", "ppt_templates.json")
                dump("SELECT id, template_name, description, html_template, preview_image, style_config, tags, is_default, is_active, usage_count, created_by, created_at, updated_at FROM global_master_templates", "global_master_templates.json")
            except Exception as e:
                logger.warning(f"âš ï¸ light ephemeral read db failed: {e}")
            finally:
                try:
                    conn.close()
                except Exception:
                    pass
        else:
            logger.warning("âš ï¸ flowslide.db ä¸å­˜åœ¨ï¼Œlight ä¸´æ—¶åŒ…ä¸å«æ•°æ®è¡¨")

        # é…ç½®æ–‡ä»¶ç®€è¦æ”¶é›†
        config_payload = {"root_files": {}, "src_config": {}}
        for name in [".env", "pyproject.toml", "uv.toml"]:
            p = Path(name)
            if p.exists():
                try:
                    txt = p.read_text(encoding='utf-8', errors='ignore')
                    if name == '.env':
                        try:
                            txt = self._filter_env_content(txt)
                        except Exception as _fe:
                            logger.warning(f"è½»é‡ä¸´æ—¶åŒ… .env è¿‡æ»¤å¤±è´¥: {_fe}")
                    config_payload["root_files"][name] = txt
                except Exception:
                    pass
        cfg_dir = Path("./src/config")
        if cfg_dir.exists():
            for fp in cfg_dir.rglob('*'):
                if fp.is_file() and fp.suffix.lower() in ('.json', '.yaml', '.yml', '.toml'):
                    rel = str(fp.relative_to(cfg_dir))
                    try:
                        config_payload["src_config"][rel] = fp.read_text(encoding='utf-8', errors='ignore')
                    except Exception:
                        pass
        (data_dir / 'config_files.json').write_text(json.dumps(config_payload, ensure_ascii=False, indent=2), encoding='utf-8')

        manifest = {
            "type": "light",
            "schema_version": 1,
            "generated_at": datetime.now().isoformat(),
            "description": "Lightweight selective dataset (users/projects/templates/config)",
            "tables": ["users", "projects", "ppt_templates", "global_master_templates"],
        }
        (base / 'light_manifest.json').write_text(json.dumps(manifest, ensure_ascii=False, indent=2), encoding='utf-8')

        zip_path = Path(tmp_dir_ctx.name) / archive_name
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
            for p in base.rglob('*'):
                if p.is_file():
                    zf.write(p, p.relative_to(base.parent))
        logger.info(f"ğŸª¶ Created ephemeral light archive: {zip_path}")
        # è¿”å› (zip_path, tmp_dir_ctx) ç”±è°ƒç”¨æ–¹æŒæœ‰ tmp_dir_ctx é˜²æ­¢æå‰æ¸…ç†
        return (zip_path, tmp_dir_ctx)

    async def upload_light_ephemeral(self, archive_path: Path) -> Dict[str, Any]:
        """ä¸Šä¼  light ä¸´æ—¶åŒ…åˆ° R2ï¼Œåªä¿ç•™ latest ä¸ backup ä¸¤ä¸ªå¯¹è±¡ã€‚"""
        if not self._is_r2_configured():
            raise Exception("R2æœªé…ç½®")
        from botocore.config import Config
        cfg = Config(region_name='auto', retries={'max_attempts':3,'mode':'standard'})
        s3 = boto3.client(
            's3',
            aws_access_key_id=self.r2_config['access_key'],
            aws_secret_access_key=self.r2_config['secret_key'],
            endpoint_url=self.r2_config['endpoint'],
            config=cfg
        )
        bucket = self.r2_config['bucket']
        prefix = 'backups/light/'
        latest_key = prefix + 'flowslide_light_latest.zip'
        backup_key = prefix + 'flowslide_light_backup.zip'

        async def _exists(k: str) -> bool:
            try:
                await asyncio.to_thread(s3.head_object, Bucket=bucket, Key=k)
                return True
            except Exception:
                return False

        # åˆ é™¤æ—§ backup
        try:
            await asyncio.to_thread(s3.delete_object, Bucket=bucket, Key=backup_key)
        except Exception:
            pass

        # latest -> backup
        if await _exists(latest_key):
            try:
                await asyncio.to_thread(
                    s3.copy_object,
                    Bucket=bucket,
                    CopySource={'Bucket': bucket, 'Key': latest_key},
                    Key=backup_key
                )
                await asyncio.to_thread(s3.delete_object, Bucket=bucket, Key=latest_key)
            except Exception as e:
                logger.warning(f"å¤åˆ¶ latest->backup å¤±è´¥: {e}")

        # ä¸Šä¼ æ–° latest
        await asyncio.to_thread(s3.upload_file, str(archive_path), bucket, latest_key)

        # æ¸…ç†å…¶å®ƒåŒå‰ç¼€å¯¹è±¡
        try:
            resp = await asyncio.to_thread(s3.list_objects_v2, Bucket=bucket, Prefix=prefix)
            allowed = {latest_key, backup_key}
            for obj in resp.get('Contents', []) if resp else []:
                k = obj.get('Key')
                if k and k not in allowed:
                    try:
                        await asyncio.to_thread(s3.delete_object, Bucket=bucket, Key=k)
                    except Exception:
                        pass
        except Exception:
            pass

        info = {
            'filename': 'flowslide_light_latest.zip',
            'size': archive_path.stat().st_size if archive_path.exists() else None,
            'timestamp': datetime.now().isoformat(),
            'r2_key_latest': latest_key,
            'r2_key_backup': backup_key,
            'success': True
        }
        logger.info("âœ… Light archive uploaded with rotation (latest + backup kept)")
        return info

    async def _create_light_backup_archive(self, backup_path: Path) -> Path:
        """åˆ›å»ºè½»é‡çº§å¤‡ä»½å‹ç¼©åŒ… (ä»…ç»“æ„åŒ–ä¸šåŠ¡æ•°æ® JSON)ã€‚

        å†…å®¹ç»“æ„ï¼š
        - light_manifest.json : å…ƒæ•°æ®ä¸ç‰ˆæœ¬
        - data/users.json
        - data/projects.json
        - data/ppt_templates.json
        - data/global_master_templates.json
        - data/config_files.json  (å¦‚æœå¤–éƒ¨åŒæ­¥è¡¨å­˜åœ¨å¹¶æœ‰å†…å®¹ / æˆ–æœ¬åœ° src/config & æ ¹éƒ¨é…ç½®æ–‡ä»¶)
        
        æ³¨æ„ï¼šä¸åŒ…å« uploads / research_reports / è„šæœ¬ / æ•´ä¸ªæ•°æ®åº“æ–‡ä»¶ï¼Œä»¥ä¾¿ç”¨äºå¿«é€Ÿâ€œåˆå¹¶å¼â€æ¢å¤æˆ–è¿ç§»ã€‚
        """
        import json, sqlite3, zipfile
        data_dir = backup_path / "data"
        data_dir.mkdir(parents=True, exist_ok=True)

        # è¯»å–æœ¬åœ° SQLite æ•°æ®åº“ (å¦‚æœå­˜åœ¨)ã€‚è‹¥æœªæ¥éœ€è¦æ”¯æŒç›´æ¥æŸ¥è¯¢å¤–éƒ¨DBï¼Œå¯æ‰©å±•ä¸ºæ ¹æ®é…ç½®é€‰æ‹©æ¥æºã€‚
        db_file = Path("./data/flowslide.db")
        if not db_file.exists():
            logger.warning("âš ï¸ æœ¬åœ° flowslide.db ä¸å­˜åœ¨ï¼Œlight å¤‡ä»½å°†åªåŒ…å«é…ç½®æ–‡ä»¶")
        else:
            try:
                conn = sqlite3.connect(str(db_file))
                conn.row_factory = sqlite3.Row
                cur = conn.cursor()
                def dump_table(query: str, out_name: str):
                    cur.execute(query)
                    rows = [dict(r) for r in cur.fetchall()]
                    with open(data_dir / out_name, 'w', encoding='utf-8') as f:
                        json.dump(rows, f, ensure_ascii=False, indent=2)
                    logger.info(f"ğŸ—‚ï¸ light backup wrote {out_name} ({len(rows)} rows)")

                # users (æœ€å°å­—æ®µé›†åˆå³å¯ï¼Œä¿æŒåˆ—åä¸€è‡´ä»¥ä¾¿æœªæ¥åˆå¹¶ï¼Œæ’é™¤å¯†ç hash? -> ä¿ç•™hash æ‰èƒ½æ— ç¼ç™»é™†)
                dump_table("SELECT id, username, email, is_active, is_admin, created_at, updated_at, last_login, password_hash FROM users", "users.json")
                # projects (æ ¸å¿ƒå†…å®¹: project_id åŠå…³é”®å­—æ®µï¼Œslides_html/slides_data ä¿ç•™)
                dump_table("SELECT id, project_id, title, scenario, topic, requirements, status, owner_id, outline, slides_html, slides_data, confirmed_requirements, project_metadata, version, created_at, updated_at FROM projects", "projects.json")
                # ppt_templates
                dump_table("SELECT id, project_id, template_type, template_name, description, html_template, applicable_scenarios, style_config, usage_count, created_at, updated_at FROM ppt_templates", "ppt_templates.json")
                # global_master_templates
                dump_table("SELECT id, template_name, description, html_template, preview_image, style_config, tags, is_default, is_active, usage_count, created_by, created_at, updated_at FROM global_master_templates", "global_master_templates.json")
            except Exception as e:
                logger.warning(f"âš ï¸ è¯»å– SQLite ç”Ÿæˆ light æ•°æ®å¤±è´¥: {e}")
            finally:
                try:
                    conn.close()
                except Exception:
                    pass

        # é…ç½®æ–‡ä»¶æ”¶é›† (æœ¬åœ°æ–‡ä»¶)ã€‚ç®€åŒ–å¤„ç†ï¼šæ‰“åŒ… src/config ä¸‹çš„ json/yaml ä»¥åŠæ ¹ç›®å½• *.toml / .env
        config_payload = {
            "root_files": {},
            "src_config": {},
        }
        try:
            # æ ¹ç›®å½•
            for name in [".env", "pyproject.toml", "uv.toml"]:
                p = Path(name)
                if p.exists():
                    try:
                        config_payload["root_files"][name] = p.read_text(encoding='utf-8', errors='ignore')
                    except Exception:
                        pass
            # src/config
            cfg_dir = Path("./src/config")
            if cfg_dir.exists():
                for fp in cfg_dir.rglob('*'):
                    if fp.is_file() and fp.suffix.lower() in ('.json', '.yaml', '.yml', '.toml'):
                        rel = str(fp.relative_to(cfg_dir))
                        try:
                            config_payload["src_config"][rel] = fp.read_text(encoding='utf-8', errors='ignore')
                        except Exception:
                            pass
        except Exception as e:
            logger.warning(f"âš ï¸ æ”¶é›†é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

        with open(data_dir / 'config_files.json', 'w', encoding='utf-8') as f:
            json.dump(config_payload, f, ensure_ascii=False, indent=2)

        manifest = {
            "type": "light",
            "schema_version": 1,
            "generated_at": datetime.now().isoformat(),
            "description": "Lightweight selective dataset (users/projects/templates/config)",
            "tables": ["users", "projects", "ppt_templates", "global_master_templates"],
        }
        with open(backup_path / 'light_manifest.json', 'w', encoding='utf-8') as mf:
            json.dump(manifest, mf, ensure_ascii=False, indent=2)

        # ç”Ÿæˆ zip: åªæ‰“åŒ… light_manifest.json ä¸ data ç›®å½•
        archive_path = backup_path.with_suffix('.zip')
        import zipfile
        with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zf:
            for path in backup_path.rglob('*'):
                if path.is_file():
                    zf.write(path, path.relative_to(backup_path.parent))
        shutil.rmtree(backup_path)
        logger.info(f"ğŸ—œï¸ Light backup compressed: {archive_path}")
        return archive_path

    async def create_external_sql_backup(self) -> str:
        """ä»…å¯¼å‡ºå¤–éƒ¨æ•°æ®åº“çš„ SQLï¼ˆä¸ä¸Šä¼ åˆ° R2ï¼‰ã€‚

        Returns:
            ç”Ÿæˆçš„ zip æ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"flowslide_external_sql_{timestamp}"
        backup_path = self.backup_dir / backup_name
        backup_path.mkdir(exist_ok=True)

        try:
            # strict=True: external SQL-only requires pg_dump to succeed
            await self._backup_external_database(backup_path, None, strict=True)
            archive_path = await self._compress_backup(backup_path)
            # ä¸ä¸Šä¼ åˆ° R2
            return str(archive_path)
        except Exception as e:
            logger.error(f"âŒ External SQL-only backup failed: {e}")
            raise

    async def _backup_database(self, backup_path: Path):
        """å¤‡ä»½æ•°æ®åº“"""
        try:
            # æ€»æ˜¯ä¼˜å…ˆå¤‡ä»½æœ¬åœ°SQLiteï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            db_file = Path("./data/flowslide.db")
            if db_file.exists():
                shutil.copy2(db_file, backup_path / "flowslide.db")
                logger.info("ğŸ’¾ Local SQLite database backup completed")

            # å¦‚é…ç½®äº†å¤–éƒ¨æ•°æ®åº“ï¼Œåˆ™é¢å¤–å¤‡ä»½å¤–éƒ¨æ•°æ®åº“
            try:
                from ..core.simple_config import EXTERNAL_DATABASE_URL
            except Exception:
                EXTERNAL_DATABASE_URL = os.getenv("DATABASE_URL", "")

            if EXTERNAL_DATABASE_URL and EXTERNAL_DATABASE_URL.startswith("postgres"):
                # strict=False: å¸¸è§„å¤‡ä»½ä¸­ç¼ºå°‘ pg_dump ä¸åº”å¯¼è‡´æ•´ä¸ªå¤‡ä»½å¤±è´¥
                await self._backup_external_database(backup_path, EXTERNAL_DATABASE_URL, strict=False)

        except Exception as e:
            logger.error(f"âŒ Database backup failed: {e}")
            raise

    async def _backup_external_database(self, backup_path: Path, external_url: Optional[str] = None, strict: bool = False):
        """å¤‡ä»½å¤–éƒ¨æ•°æ®åº“

        Behavior:
        - Prefer pg_dump for PostgreSQL.
        - If pg_dump is unavailable or fails and strict=True, fallback to Python-native COPY CSV export via psycopg2.
        - When strict=False, quietly skip external export on errors to not block other backup content.
        """
        try:
            # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥URLï¼Œå…¶æ¬¡ä»é…ç½®è·å–
            if not external_url:
                try:
                    from ..core.simple_config import EXTERNAL_DATABASE_URL as CFG_URL
                    external_url = CFG_URL
                except Exception:
                    external_url = os.getenv("DATABASE_URL", "")

            if not external_url:
                msg = "â„¹ï¸ No EXTERNAL_DATABASE_URL configured; skipping external DB backup"
                if strict:
                    raise RuntimeError("EXTERNAL_DATABASE_URL not configured for external SQL backup")
                logger.info(msg)
                return

            if not (external_url.startswith("postgresql://") or external_url.startswith("postgres://")):
                msg = "â„¹ï¸ External DB URL is not PostgreSQL; skipping pg_dump backup"
                if strict:
                    raise RuntimeError("External SQL backup only supports PostgreSQL URLs")
                logger.info(msg)
                return

            # å°è¯•å®šä½ pg_dump
            pg_dump_path = os.getenv("PG_DUMP_PATH")  # å¯æ˜¾å¼é…ç½®
            # å¦‚æœæ˜¾å¼è®¾ç½®äº†è·¯å¾„ä½†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™è§†ä¸ºæœªæ‰¾åˆ°
            if pg_dump_path and not os.path.isfile(pg_dump_path):
                pg_dump_path = None
            if not pg_dump_path:
                # ä½¿ç”¨ç³»ç»ŸPATHæŸ¥æ‰¾
                pg_dump_path = shutil.which("pg_dump")

            if not pg_dump_path:
                msg = (
                    "pg_dump not found. Please install PostgreSQL client tools and set PG_DUMP_PATH, or add pg_dump to PATH."
                )
                if strict:
                    logger.warning(f"{msg} Attempting Python-native COPY CSV fallback...")
                    try:
                        await self._export_external_postgres_copy(backup_path, external_url)
                        logger.info("âœ… External database exported via COPY CSV fallback")
                        return
                    except Exception as fallback_err:
                        raise RuntimeError(f"External export failed and no pg_dump: {fallback_err}")
                logger.warning(f"{msg} Skipping external SQL export for this backup.")
                return

            # è§£æURLï¼Œå°½é‡é¿å…å°†å¯†ç æš´éœ²åœ¨å‘½ä»¤è¡Œï¼ˆä½¿ç”¨ç¯å¢ƒå˜é‡PGPASSWORDï¼‰
            from urllib.parse import urlparse, parse_qs

            parsed = urlparse(external_url)
            username = parsed.username or "postgres"
            password = parsed.password or ""
            host = parsed.hostname or "localhost"
            port = str(parsed.port or 5432)
            dbname = parsed.path.lstrip("/") or "postgres"
            qs = parse_qs(parsed.query or "")
            sslmode = (qs.get("sslmode", [None])[0]) or ("require" if ("supabase" in (parsed.hostname or "") or "pooler.supabase.com" in external_url) else None)

            # Supabase æç¤ºï¼špg_dump éœ€è¦ç›´è¿æ•°æ®åº“å®ä¾‹ï¼Œä¸å»ºè®®ä½¿ç”¨ pooler ä¸»æœº
            if parsed.hostname and "pooler.supabase.com" in parsed.hostname:
                logger.warning("âš ï¸ Detected Supabase pooler host; pg_dump may fail. Prefer the direct db.<project>.supabase.co host for dumps.")

            # è¾“å‡ºæ–‡ä»¶
            out_file = backup_path / f"external_{dbname}.sql"

            # æ„å»ºå‘½ä»¤
            cmd = [
                pg_dump_path,
                "-h", host,
                "-p", port,
                "-U", username,
                "-d", dbname,
                "-F", "p",  # plain SQL
                "--no-owner",
                "--no-privileges",
                "-f", str(out_file),
            ]

            # ç¯å¢ƒå˜é‡ï¼šPGPASSWORD / PGSSLMODE
            env = os.environ.copy()
            if password:
                env["PGPASSWORD"] = password
            if sslmode:
                env["PGSSLMODE"] = sslmode

            logger.info(f"ğŸ›¸ Running pg_dump for external DB '{dbname}' on {host}:{port} -> {out_file.name}")

            def _run_dump():
                result = subprocess.run(cmd, env=env, capture_output=True, text=True)
                return result

            result = await asyncio.to_thread(_run_dump)

            if result.returncode != 0:
                stderr = (result.stderr or "").strip()
                stdout = (result.stdout or "").strip()
                if strict:
                    logger.warning(f"pg_dump failed (code {result.returncode}): {stderr or stdout}. Attempting COPY CSV fallback...")
                    try:
                        await self._export_external_postgres_copy(backup_path, external_url)
                        logger.info("âœ… External database exported via COPY CSV fallback")
                        return
                    except Exception as fallback_err:
                        raise RuntimeError(f"pg_dump failed and COPY fallback failed: {fallback_err}")
                logger.warning(f"pg_dump failed (code {result.returncode}), skipping external SQL export: {stderr or stdout}")
                return

            if not out_file.exists() or out_file.stat().st_size == 0:
                if strict:
                    logger.warning("pg_dump produced no output file; attempting COPY CSV fallback...")
                    try:
                        await self._export_external_postgres_copy(backup_path, external_url)
                        logger.info("âœ… External database exported via COPY CSV fallback")
                        return
                    except Exception as fallback_err:
                        raise RuntimeError(f"pg_dump produced no output and COPY fallback failed: {fallback_err}")
                logger.warning("pg_dump produced no output file; skipping external SQL export")
                return

            logger.info(f"âœ… External PostgreSQL dump completed: {out_file} ({out_file.stat().st_size} bytes)")

        except Exception as e:
            logger.error(f"âŒ External database backup failed: {e}")
            # æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨æ–¹å†³å®šæ˜¯å¦ç»§ç»­
            raise

    async def _export_external_postgres_copy(self, backup_path: Path, external_url: str) -> None:
        """ä½¿ç”¨ psycopg2 æ‰§è¡Œ PostgreSQL çš„ CSV å¯¼å‡ºï¼ˆæ•°æ®-onlyï¼Œschema ä¸åŒ…å«ï¼‰ã€‚

        ç”Ÿæˆæ–‡ä»¶ï¼š
        - tables/<schema>.<table>.csv æ¯ä¸ªè¡¨ä¸€ä»½ CSVï¼ˆUTF-8ï¼Œå¸¦è¡¨å¤´ï¼‰
        - external_copy_manifest.json å…ƒæ•°æ®ï¼ˆè¡¨æ¸…å•ã€å¯¼å‡ºæ—¶é—´ã€è¿æ¥ä¸»æœº/åº“åï¼‰
        """
        import json
        from urllib.parse import urlparse, parse_qs
        try:
            import psycopg2
        except Exception as ie:
            raise RuntimeError(f"psycopg2 not available: {ie}")

        parsed = urlparse(external_url)
        username = parsed.username or "postgres"
        password = parsed.password or ""
        host = parsed.hostname or "localhost"
        port = parsed.port or 5432
        dbname = parsed.path.lstrip("/") or "postgres"
        qs = parse_qs(parsed.query or "")
        sslmode = (qs.get("sslmode", [None])[0]) or None

        conn_kwargs = {
            "user": username,
            "password": password,
            "host": host,
            "port": port,
            "dbname": dbname,
        }
        if sslmode:
            conn_kwargs["sslmode"] = sslmode

        tables_dir = backup_path / "tables"
        tables_dir.mkdir(exist_ok=True)

        def _run_copy():
            with psycopg2.connect(**conn_kwargs) as conn:
                conn.autocommit = True
                with conn.cursor() as cur:
                    # è·å–ç”¨æˆ·è¡¨åˆ—è¡¨ï¼ˆæ’é™¤ç³»ç»Ÿ schemaï¼‰
                    cur.execute(
                        """
                        SELECT table_schema, table_name
                        FROM information_schema.tables
                        WHERE table_type='BASE TABLE'
                          AND table_schema NOT IN ('pg_catalog','information_schema')
                        ORDER BY table_schema, table_name
                        """
                    )
                    rows = cur.fetchall()
                    exported = []
                    for schema, table in rows:
                        safe_name = f"{schema}.{table}"
                        out_path = tables_dir / f"{schema}.{table}.csv"
                        sql = f"COPY \"{schema}\".\"{table}\" TO STDOUT WITH CSV HEADER"
                        with open(out_path, "w", encoding="utf-8", newline="") as f:
                            cur.copy_expert(sql, f)
                        exported.append({"schema": schema, "table": table, "file": out_path.name})

            # å†™å…¥æ¸…å•
            manifest = {
                "database": dbname,
                "host": host,
                "port": port,
                "exported_tables": exported,
                "exported_at": datetime.now().isoformat(),
                "format": "csv",
                "note": "Data-only export; schema DDL not included."
            }
            with open(backup_path / "external_copy_manifest.json", "w", encoding="utf-8") as mf:
                json.dump(manifest, mf, ensure_ascii=False, indent=2)

        # Run in thread to avoid blocking loop
        await asyncio.to_thread(_run_copy)

    async def _backup_config(self, backup_path: Path):
        """å¤‡ä»½é…ç½®æ–‡ä»¶"""
        config_files = [".env", "pyproject.toml", "uv.toml"]

        for config_file in config_files:
            p = Path(config_file)
            if not p.exists():
                continue
            try:
                if p.name == '.env':
                    # ç›´æ¥æ˜æ–‡å¤åˆ¶ .env
                    shutil.copy2(p, backup_path / p.name)
                else:
                    shutil.copy2(p, backup_path / p.name)
            except Exception as ce:
                logger.warning(f"è·³è¿‡é…ç½®æ–‡ä»¶ {config_file}: {ce}")

        # è‹¥ .env ä¸ºç©ºæ–‡ä»¶ï¼Œé¢å¤–ç”Ÿæˆä¸€ä¸ªè¿è¡Œæ—¶ç¯å¢ƒå¿«ç…§ï¼Œé¿å…ç”¨æˆ·è¯¯ä»¥ä¸ºä¸¢å¤±
        try:
            env_file = backup_path / '.env'
            if (not env_file.exists()) or env_file.stat().st_size == 0:
                snapshot_path = backup_path / 'env_runtime_snapshot.txt'
                import os as _os
                lines = []
                for k,v in sorted(_os.environ.items()):
                    if any(s in k for s in ("KEY","SECRET","TOKEN","PASSWORD")):
                        # åªä¿ç•™ key åç§°ï¼Œä¸æš´éœ²æ•æ„Ÿå€¼
                        lines.append(f"{k}=***redacted***")
                    else:
                        lines.append(f"{k}={v}")
                snapshot_path.write_text("\n".join(lines), encoding='utf-8')
                logger.info("ğŸ§¾ Generated env_runtime_snapshot.txt (sanitized)")
        except Exception as se:
            logger.warning(f"ç”Ÿæˆç¯å¢ƒå¿«ç…§å¤±è´¥: {se}")

        logger.info("âš™ï¸ Config backup completed")

        # é™„åŠ ï¼šå¤åˆ¶ src/config ä¸‹çš„é…ç½®ï¼ˆè‹¥å­˜åœ¨ï¼‰
        cfg_dir = Path("./src/config")
        if cfg_dir.exists() and cfg_dir.is_dir():
            dest = backup_path / "src_config"
            shutil.copytree(cfg_dir, dest, dirs_exist_ok=True)
            logger.info("ğŸ“ src/config included in config backup")

    # ================== ç¯å¢ƒå˜é‡ç™½åå• & è¿‡æ»¤å·¥å…· ==================
    # ä¿ç•™ç©ºå‡½æ•°ä»¥é˜²å¤–éƒ¨è°ƒç”¨å¼•ç”¨ï¼Œç›´æ¥è¿”å›åŸæ–‡æœ¬
    def _filter_env_content(self, text: str) -> str:
        return text

    async def _backup_uploads(self, backup_path: Path):
        """å¤‡ä»½ä¸Šä¼ æ–‡ä»¶"""
        uploads_dir = Path("./uploads")
        if uploads_dir.exists():
            shutil.copytree(uploads_dir, backup_path / "uploads", dirs_exist_ok=True)
            logger.info("ğŸ“ Uploads backup completed")

    async def _backup_templates(self, backup_path: Path):
        """å¤‡ä»½æ¨¡æ¿ç¤ºä¾‹"""
        t_dir = Path("./template_examples")
        if t_dir.exists():
            shutil.copytree(t_dir, backup_path / "template_examples", dirs_exist_ok=True)
            logger.info("ğŸ“š Template examples backup completed")

    async def _backup_reports(self, backup_path: Path):
        """å¤‡ä»½ç ”ç©¶æŠ¥å‘Š"""
        r_dir = Path("./research_reports")
        if r_dir.exists():
            shutil.copytree(r_dir, backup_path / "research_reports", dirs_exist_ok=True)
            logger.info("ğŸ“‘ Research reports backup completed")

    async def _backup_scripts(self, backup_path: Path):
        """å¤‡ä»½è„šæœ¬"""
        s_dir = Path("./scripts")
        if s_dir.exists():
            shutil.copytree(s_dir, backup_path / "scripts", dirs_exist_ok=True)
            logger.info("ğŸ”§ Scripts backup completed")

    async def _compress_backup(self, backup_path: Path) -> Path:
        """å‹ç¼©å¤‡ä»½æ–‡ä»¶"""
        import zipfile

        archive_path = backup_path.with_suffix('.zip')

        with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_path in backup_path.rglob('*'):
                if file_path.is_file():
                    zipf.write(file_path, file_path.relative_to(backup_path.parent))

        # åˆ é™¤åŸå§‹å¤‡ä»½ç›®å½•
        shutil.rmtree(backup_path)

        logger.info(f"ğŸ—œï¸ Backup compressed: {archive_path}")
        return archive_path

    async def _upload_to_r2(self, backup_path: Path):
        """ä¸Šä¼ å¤‡ä»½åˆ°R2"""
        if not self._is_r2_configured():
            logger.info("â„¹ï¸ R2 not configured, skipping cloud backup")
            return

        try:
            logger.info(f"â˜ï¸ Starting R2 upload: {backup_path.name}")

            # åˆ›å»ºS3å®¢æˆ·ç«¯ï¼Œé…ç½®ä¸ºR2
            from botocore.config import Config
            config = Config(
                region_name='auto',
                retries={'max_attempts': 3, 'mode': 'standard'},
                read_timeout=30,
                connect_timeout=15,
                signature_version='s3v4'
            )

            s3_client = boto3.client(
                's3',
                aws_access_key_id=self.r2_config['access_key'],
                aws_secret_access_key=self.r2_config['secret_key'],
                endpoint_url=self.r2_config['endpoint'],
                config=config
            )

            # ç”ŸæˆæŒ‰ç±»åˆ«çš„å‰ç¼€ä¸æ—¥æœŸç›®å½•
            backup_date = datetime.now().strftime("%Y-%m-%d")
            # æœŸæœ›æ–‡ä»¶åæ ¼å¼ï¼šflowslide_backup_{type}_YYYYMMDD_HHMMSS.zip
            name = backup_path.name
            type_segment = "misc"
            try:
                # flowslide_backup_ + rest
                rest = name[len("flowslide_backup_"):]
                type_segment = rest.split("_")[0] or "misc"
            except Exception:
                type_segment = "misc"

            # å°† db_only å½’ç±»åˆ° database å‰ç¼€ï¼›å…¶ä»–èµ° categories/{type}
            if type_segment == "db_only":
                s3_key = f"backups/database/{backup_date}/{backup_path.name}"
            else:
                s3_key = f"backups/categories/{type_segment}/{backup_date}/{backup_path.name}"

            # ä¸Šä¼ æ–‡ä»¶
            logger.info(f"Uploading to R2: {self.r2_config['bucket']}/{s3_key}")

            # ä½¿ç”¨asyncio.to_threadåœ¨åå°çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥çš„boto3æ“ä½œ
            await asyncio.to_thread(
                s3_client.upload_file,
                str(backup_path),
                self.r2_config['bucket'],
                s3_key
            )

            logger.info(f"âœ… R2 upload completed successfully: {backup_path.name}")

        except ClientError as e:
            error_code = e.response['Error']['Code']
            error_msg = e.response['Error']['Message']
            logger.error(f"âŒ R2 upload failed (AWS Error {error_code}): {error_msg}")
            raise Exception(f"R2 upload failed: {error_msg}")
        except Exception as e:
            logger.error(f"âŒ R2 upload failed: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºæœ¬åœ°å¤‡ä»½å·²ç»æˆåŠŸ

    def _is_r2_configured(self) -> bool:
        """æ£€æŸ¥R2æ˜¯å¦é…ç½®ï¼ˆæ¯æ¬¡è°ƒç”¨éƒ½ä¼šåˆ·æ–°ç¯å¢ƒå˜é‡ï¼‰"""
        self.refresh_r2_config()
        return all(self.r2_config.values())

    async def _cleanup_old_backups(self):
        """æ¸…ç†æ—§å¤‡ä»½"""
        try:
            cutoff_date = datetime.now().timestamp() - (self.retention_days * 24 * 60 * 60)

            for backup_file in self.backup_dir.glob("*.zip"):
                if backup_file.stat().st_mtime < cutoff_date:
                    backup_file.unlink()
                    logger.info(f"ğŸ—‘ï¸ Cleaned up old backup: {backup_file.name}")

        except Exception as e:
            logger.error(f"âŒ Cleanup failed: {e}")

    async def _send_notification(self, backup_name: str, status: str, error: Optional[str] = None):
        """å‘é€å¤‡ä»½é€šçŸ¥"""
        if not self.webhook_url:
            return

        try:
            import aiohttp

            message = {
                "backup_name": backup_name,
                "status": status,
                "timestamp": datetime.now().isoformat(),
                "error": error
            }

            async with aiohttp.ClientSession() as session:
                await session.post(self.webhook_url, json=message)

        except Exception as e:
            logger.error(f"âŒ Notification failed: {e}")

    async def sync_to_r2(self, backup_path: Optional[Path] = None) -> Dict[str, Any]:
        """åŒæ­¥å¤‡ä»½åˆ°R2äº‘å­˜å‚¨

        Args:
            backup_path: æŒ‡å®šçš„å¤‡ä»½æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ€æ–°çš„å¤‡ä»½

        Returns:
            åŒæ­¥ç»“æœä¿¡æ¯
        """
        if not self._is_r2_configured():
            raise Exception("R2äº‘å­˜å‚¨æœªé…ç½®")

        try:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šå¤‡ä»½è·¯å¾„ï¼Œä½¿ç”¨æœ€æ–°çš„å¤‡ä»½
            if backup_path is None:
                backups = await list_backups()
                if not backups:
                    raise Exception("æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶è¿›è¡ŒåŒæ­¥")
                backup_path = Path(backups[0]["path"])

            # ç¡®ä¿å¤‡ä»½æ–‡ä»¶å­˜åœ¨
            if not backup_path.exists():
                raise FileNotFoundError(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_path}")

            logger.info(f"â˜ï¸ Starting R2 sync: {backup_path.name}")

            # ä¸Šä¼ åˆ°R2
            await self._upload_to_r2(backup_path)

            sync_info = {
                "filename": backup_path.name,
                "size": backup_path.stat().st_size,
                "timestamp": datetime.now().isoformat(),
                # created_at is the file mtime (when the backup file was created)
                "created_at": datetime.fromtimestamp(backup_path.stat().st_mtime).isoformat(),
                "success": True
            }

            logger.info(f"âœ… R2 sync completed: {backup_path.name}")
            return sync_info

        except Exception as e:
            logger.error(f"âŒ R2 sync failed: {e}")
            raise

    def list_backups(self) -> list:
        """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½"""
        backups = []
        for backup_file in self.backup_dir.glob("*.zip"):
            stat = backup_file.stat()
            backups.append({
                "name": backup_file.name,
                "size": stat.st_size,
                "created": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                "path": str(backup_file)
            })

        return sorted(backups, key=lambda x: x["created"], reverse=True)

    async def restore_backup(self, backup_name: str) -> bool:
        """æ¢å¤å¤‡ä»½"""
        backup_path = self.backup_dir / backup_name
        if not backup_path.exists():
            raise FileNotFoundError(f"Backup not found: {backup_name}")

        try:
            logger.info(f"ğŸ”„ Restoring backup: {backup_name}")

            # åˆ›å»ºä¸´æ—¶æ¢å¤ç›®å½•
            restore_temp_dir = self.backup_dir / f"restore_temp_{int(time.time())}"
            restore_temp_dir.mkdir(exist_ok=True)

            def extract_and_restore(backup_name_inner: str = backup_name):
                import zipfile
                import shutil
                from pathlib import Path

                try:
                    # è§£å‹å¤‡ä»½æ–‡ä»¶
                    logger.info(f"ğŸ“¦ Extracting backup: {backup_name_inner}")
                    with zipfile.ZipFile(str(backup_path), 'r') as zip_ref:
                        zip_ref.extractall(str(restore_temp_dir))

                    # æ£€æµ‹æ˜¯å¦ä¸ºè½»é‡å¤‡ä»½ï¼ˆå­˜åœ¨ light_manifest.jsonï¼‰
                    light_manifest = list(restore_temp_dir.rglob("light_manifest.json"))
                    is_light = bool(light_manifest)
                    if is_light:
                        logger.info("ğŸª¶ Detected light backup manifest; executing merge restore logic (ä¸ä¼šæ•´ä½“æ›¿æ¢æ•°æ®åº“æ–‡ä»¶)")
                    
                    db_files = list(restore_temp_dir.rglob("*.db")) if not is_light else []
                    if not is_light:
                        if not db_files:
                            raise Exception("å¤‡ä»½æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æ•°æ®åº“æ–‡ä»¶")
                        db_file = db_files[0]
                        logger.info(f"ğŸ—„ï¸ Found database file: {db_file.name} at {db_file}")
                    else:
                        db_file = None

                    current_db_path = Path("./data/flowslide.db")
                    if not is_light:
                        # å¤‡ä»½å½“å‰æ•°æ®åº“
                        if current_db_path.exists():
                            backup_current = current_db_path.with_suffix('.db.backup')
                            shutil.copy2(str(current_db_path), str(backup_current))
                            logger.info(f"ğŸ’¾ Backed up current database to: {backup_current}")
                        # æ¢å¤æ•°æ®åº“æ–‡ä»¶
                        if db_file is not None:
                            shutil.copy2(str(db_file), str(current_db_path))
                            try:
                                db_display = getattr(db_file, 'name', str(db_file))
                            except Exception:
                                db_display = str(db_file)
                            logger.info(f"âœ… Database restored from: {db_display}")
                    else:
                        # è½»é‡åˆå¹¶æ¢å¤
                        self._merge_light_backup_into_sqlite(restore_temp_dir, current_db_path)
                        logger.info("âœ… Light backup merged into existing database")

                    # æ¢å¤ä¸Šä¼ æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    uploads_dirs = list(restore_temp_dir.rglob("uploads"))
                    if uploads_dirs:
                        uploads_dir = uploads_dirs[0]
                        if uploads_dir.exists() and uploads_dir.is_dir():
                            target_uploads = Path("./uploads")
                            if target_uploads.exists():
                                shutil.rmtree(str(target_uploads))
                            shutil.copytree(str(uploads_dir), str(target_uploads))
                            logger.info("ğŸ“ Uploads directory restored")

                    # æ¢å¤é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    config_files = list(restore_temp_dir.rglob("*.json")) + list(restore_temp_dir.rglob("*.yaml")) + list(restore_temp_dir.rglob("*.yml"))
                    for config_file in config_files:
                        if "flowslide" in config_file.name.lower():
                            target_config = Path(".") / config_file.name
                            shutil.copy2(str(config_file), str(target_config))
                            logger.info(f"âš™ï¸ Config file restored: {config_file.name}")

                    # æ–°å¢: æ¢å¤ .env ï¼ˆè‹¥å¤‡ä»½ä¸­å­˜åœ¨ï¼‰
                    try:
                        env_candidates = list(restore_temp_dir.rglob('.env'))
                        if env_candidates:
                            env_src = env_candidates[0]
                            env_target = Path('.env')
                            if env_target.exists():
                                # å…ˆåšå¤‡ä»½
                                backup_name = f".env.before_restore_{int(time.time())}"
                                shutil.copy2(str(env_target), backup_name)
                                logger.info(f"ğŸ›¡ï¸ Existing .env backed up as {backup_name}")
                            # è‹¥å¤‡ä»½ä¸­çš„ .env å«æœ‰ redacted è¯´æ˜æ˜¯ç™½åå•è¿‡æ»¤ç‰ˆæœ¬ -> åˆå¹¶ç­–ç•¥
                            try:
                                new_text = env_src.read_text(encoding='utf-8', errors='ignore')
                            except Exception:
                                new_text = ''
                            force_full = os.getenv('FORCE_ENV_FULL_OVERWRITE', 'true').lower() == 'true'
                            if not force_full:
                                logger.info("ğŸ”§ FORCE_ENV_FULL_OVERWRITE=false: å¯ç”¨å®‰å…¨åˆå¹¶æ¨¡å¼ (.env)")
                            if ('***redacted***' in new_text and env_target.exists() and not force_full):
                                try:
                                    existing_text = env_target.read_text(encoding='utf-8', errors='ignore') if env_target.exists() else ''
                                except Exception:
                                    existing_text = ''
                                # è§£æä¸º map
                                def parse(text:str):
                                    m = {}
                                    for line in text.splitlines():
                                        if line.strip() and not line.strip().startswith('#') and '=' in line:
                                            k,v = line.split('=',1)
                                            m[k.strip()] = v
                                    return m
                                existing_map = parse(existing_text)
                                incoming_map = parse(new_text)
                                wl = set(self._get_env_whitelist()) if hasattr(self, '_get_env_whitelist') else set()
                                merged = existing_map.copy()
                                for k,v in incoming_map.items():
                                    if k in wl:
                                        if v != '***redacted***':
                                            merged[k] = v
                                        # å¦‚æœæ˜¯ redacted ä¿ç•™åŸå€¼ï¼ˆè‹¥ä¸å­˜åœ¨åˆ™ä¸å†™å…¥ï¼‰
                                # åºåˆ—åŒ–ï¼Œä¿æŒåŸæœ‰é¡ºåºï¼ˆå…ˆç™½åå•æ’åºï¼Œåå…¶ä½™ï¼‰
                                ordered_keys = [k for k in wl if k in merged] + [k for k in merged.keys() if k not in wl]
                                lines = [f"{k}={merged[k]}" for k in ordered_keys]
                                env_target.write_text('\n'.join(lines), encoding='utf-8')
                                logger.info("ğŸ” .env restored with merge (preserved local sensitive keys)")
                            else:
                                # ç›´æ¥è¦†ç›–ï¼ˆå®Œæ•´æœªè¿‡æ»¤ç‰ˆæœ¬ï¼‰
                                shutil.copy2(str(env_src), str(env_target))
                                logger.info("ğŸ” .env restored from backup archive (direct copy)")
                    except Exception as env_e:
                        logger.warning(f"âš ï¸ .env restore skipped: {env_e}")

                    logger.info("âœ… Backup restored successfully")
                    return True

                except Exception as e:
                    logger.error(f"âŒ Restore operation failed: {e}")
                    # å°è¯•æ¢å¤åŸå§‹æ•°æ®åº“
                    current_db_path = Path("./data/flowslide.db")
                    backup_current = current_db_path.with_suffix('.db.backup')
                    if backup_current.exists():
                        shutil.copy2(str(backup_current), str(current_db_path))
                        logger.info("ğŸ”„ Original database restored from backup")
                    raise
                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if restore_temp_dir.exists():
                        shutil.rmtree(str(restore_temp_dir))
                        logger.info("ğŸ§¹ Temporary restore files cleaned up")

            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œæ¢å¤æ“ä½œ
            await asyncio.to_thread(extract_and_restore)
            return True

        except Exception as e:
            logger.error(f"âŒ Restore failed: {e}")
            return False

    # ================== åŠ¨æ€ .env è¦†ç›–æ¨¡å¼ç®¡ç† ==================
    def get_env_mode(self) -> Dict[str, Any]:
        """è¿”å›å½“å‰ .env æ¢å¤æ¨¡å¼ä¿¡æ¯ã€‚"""
        force = os.getenv('FORCE_ENV_FULL_OVERWRITE', 'true').lower() == 'true'
        mode = 'full_overwrite' if force else 'merge_whitelist'
        wl = []
        try:
            wl = self._get_env_whitelist()  # type: ignore[attr-defined]
        except Exception:
            wl = []
        return {
            'force_full': force,
            'mode': mode,
            'whitelist_count': len(wl),
            'whitelist': wl,
        }

    def set_env_mode(self, force_full: bool) -> Dict[str, Any]:
        """è®¾ç½®å¹¶æŒä¹…åŒ– .env æ¢å¤æ¨¡å¼ã€‚

        æŒä¹…åŒ–ç­–ç•¥ï¼šæ›´æ–°å½“å‰è¿›ç¨‹ç¯å¢ƒå˜é‡ + ä¿®æ”¹/è¿½åŠ  .env ä¸­çš„ FORCE_ENV_FULL_OVERWRITE= å€¼ã€‚
        """
        os.environ['FORCE_ENV_FULL_OVERWRITE'] = 'true' if force_full else 'false'
        env_path = Path('.env')
        try:
            if env_path.exists():
                try:
                    lines = env_path.read_text(encoding='utf-8', errors='ignore').splitlines()
                except Exception:
                    lines = []
                found = False
                for i,l in enumerate(lines):
                    if l.strip().startswith('FORCE_ENV_FULL_OVERWRITE='):
                        lines[i] = f"FORCE_ENV_FULL_OVERWRITE={'true' if force_full else 'false'}"
                        found = True
                        break
                if not found:
                    lines.append(f"FORCE_ENV_FULL_OVERWRITE={'true' if force_full else 'false'}")
                env_path.write_text('\n'.join(lines)+('\n' if lines else ''), encoding='utf-8')
            else:
                env_path.write_text(f"FORCE_ENV_FULL_OVERWRITE={'true' if force_full else 'false'}\n", encoding='utf-8')
        except Exception as e:
            logger.warning(f"æ›´æ–° .env æ–‡ä»¶ä¸­çš„ FORCE_ENV_FULL_OVERWRITE å¤±è´¥: {e}")
        return self.get_env_mode()

    def _merge_light_backup_into_sqlite(self, extracted_root: Path, sqlite_path: Path) -> None:
        """å°†è½»é‡å¤‡ä»½(JSONæ•°æ®)åˆå¹¶å†™å…¥ç°æœ‰SQLiteæ•°æ®åº“ã€‚

        ç­–ç•¥ï¼šå¯¹äº users / projects / ppt_templates / global_master_templates
        - è‹¥æœ¬åœ°ä¸å­˜åœ¨ id -> æ’å…¥
        - è‹¥å­˜åœ¨ id -> æ¯”è¾ƒ updated_at å­—æ®µï¼ˆæ— åˆ™ä½¿ç”¨ created_atï¼‰ï¼Œè¾ƒæ–°çš„è¦†ç›–æŒ‡å®šå­—æ®µ
        - ä¸åˆ é™¤æœ¬åœ°å·²æœ‰ä½†å¤‡ä»½ç¼ºå¤±çš„è®°å½•ï¼ˆä¿æŒå¹‚ç­‰å¢é‡ï¼‰

        é…ç½®æ–‡ä»¶ï¼šå†™å…¥åˆ°ä¸´æ—¶ç›®å½• / ä¸ç›´æ¥è¦†ç›– .env (å®‰å…¨è€ƒè™‘)ï¼›pyproject.toml ç­‰å¦‚æœä¸å­˜åœ¨åˆ™ç”Ÿæˆã€‚
        """
        import json, sqlite3
        data_dir = extracted_root / 'data'
        if not data_dir.exists():
            logger.warning("light backup data dir missing; skip merge")
            return
        if not sqlite_path.exists():
            logger.warning("SQLite database not found; cannot merge light backup")
            return
        conn = sqlite3.connect(str(sqlite_path))
        conn.row_factory = sqlite3.Row
        cur = conn.cursor()

        def load_json(name: str):
            p = data_dir / name
            if not p.exists():
                return []
            try:
                return json.loads(p.read_text(encoding='utf-8'))
            except Exception as e:
                logger.warning(f"Failed loading {name}: {e}")
                return []

        try:
            users = load_json('users.json')
            projects = load_json('projects.json')
            ppt_templates = load_json('ppt_templates.json')
            global_templates = load_json('global_master_templates.json')
            # è¯»å–é…ç½®æ–‡ä»¶èšåˆï¼ˆåŒ…å« .env å†…å®¹ï¼‰
            config_payload = None
            cfg_path = data_dir / 'config_files.json'
            if cfg_path.exists():
                try:
                    import json as _json
                    config_payload = _json.loads(cfg_path.read_text(encoding='utf-8'))
                except Exception as _e_cfg:
                    logger.warning(f"è¯»å– config_files.json å¤±è´¥: {_e_cfg}")

            def upsert(table: str, row: dict, key_field: str = 'id', timestamp_fields=("updated_at","created_at")):
                # è·å–æœ¬åœ°è®°å½•
                key = row.get(key_field)
                if key is None:
                    return
                cur.execute(f"SELECT * FROM {table} WHERE {key_field}=?", (key,))
                existing = cur.fetchone()
                def ts(r):
                    for f in timestamp_fields:
                        v = r.get(f) if isinstance(r, dict) else (r[f] if r and f in r.keys() else None)
                        if v is not None:
                            return float(v)
                    return 0.0
                if not existing:
                    # æ’å…¥
                    cols = list(row.keys())
                    placeholders = ','.join(['?']*len(cols))
                    cur.execute(f"INSERT INTO {table} ({','.join(cols)}) VALUES ({placeholders})", tuple(row[c] for c in cols))
                else:
                    local_ts = ts({k: existing[k] for k in existing.keys()})
                    remote_ts = ts(row)
                    if remote_ts > local_ts:
                        # è¦†ç›–æ›´æ–°ï¼ˆä¸æ”¹å˜ç¼ºå¤±å­—æ®µï¼‰
                        cols = [c for c in row.keys() if c != key_field]
                        set_clause = ','.join([f"{c}=?" for c in cols])
                        cur.execute(f"UPDATE {table} SET {set_clause} WHERE {key_field}=?", tuple(row[c] for c in cols)+(key,))

            for u in users:
                upsert('users', u)
            for p in projects:
                upsert('projects', p)
            for t in ppt_templates:
                upsert('ppt_templates', t)
            for gt in global_templates:
                upsert('global_master_templates', gt)

            conn.commit()
            logger.info("Light backup data merged into SQLite (users/projects/templates)")
            # åˆå¹¶æ¢å¤ .envï¼ˆå¦‚æœè½»é‡åŒ…é‡ŒåŒ…å« root_files -> .envï¼‰
            try:
                if config_payload and isinstance(config_payload, dict):
                    root_files = config_payload.get('root_files') or {}
                    env_content = root_files.get('.env')
                    if env_content is not None:
                        target = Path('.env')
                        if target.exists():
                            bak = f".env.before_light_merge_{int(time.time())}"
                            try:
                                target.write_text(target.read_text(encoding='utf-8'), encoding='utf-8')  # touch to ensure readable
                            except Exception:
                                pass
                            shutil.copy2(str(target), bak)
                            logger.info(f"ğŸ›¡ï¸ Existing .env backed up as {bak}")
                        try:
                            # ä»…åˆå¹¶ç™½åå•å˜é‡ï¼Œéç™½åå•ä¿æŒæœ¬åœ°å€¼ï¼ˆè‹¥è¿œç¨‹ä¸º ***redacted*** ç›´æ¥å¿½ç•¥ï¼‰
                            wl = set(self._get_env_whitelist()) if hasattr(self, '_get_env_whitelist') else set()
                            existing_map = {}
                            if target.exists():
                                for line in target.read_text(encoding='utf-8', errors='ignore').splitlines():
                                    if line.strip() and not line.strip().startswith('#') and '=' in line:
                                        k,v = line.split('=',1)
                                        existing_map[k.strip()] = v
                            merged = []
                            for line in env_content.splitlines():
                                if line.strip().startswith('#') or '=' not in line:
                                    merged.append(line)
                                    continue
                                k,v = line.split('=',1)
                                ks = k.strip()
                                if ks in wl:
                                    if v == '***redacted***':
                                        # ä¿æŒç°æœ‰ï¼Œè‹¥æ²¡æœ‰åˆ™è·³è¿‡
                                        if ks in existing_map:
                                            merged.append(f"{ks}={existing_map[ks]}")
                                    else:
                                        merged.append(f"{ks}={v}")
                                else:
                                    # éç™½åå•ä¿æŒåŸæœ‰
                                    if ks in existing_map:
                                        merged.append(f"{ks}={existing_map[ks]}")
                            # æ·»åŠ å‰©ä½™æœªå†™å…¥çš„æœ¬åœ°éç™½åå•å˜é‡
                            for k,v in existing_map.items():
                                if not any(l.startswith(f"{k}=") for l in merged):
                                    merged.append(f"{k}={v}")
                            target.write_text('\n'.join(merged), encoding='utf-8')
                            logger.info("ğŸ” .env restored with whitelist merge (light)")
                        except Exception as w_e:
                            logger.warning(f"âš ï¸ å†™å…¥ .env (whitelist merge) å¤±è´¥: {w_e}")
            except Exception as e_env_merge:
                logger.warning(f"âš ï¸ è½»é‡æ¢å¤ .env å¤„ç†å¼‚å¸¸: {e_env_merge}")
        except Exception as e:
            logger.error(f"Merge light backup failed: {e}")
        finally:
            try:
                conn.close()
            except Exception:
                pass

    async def restore_from_r2(self) -> Dict[str, Any]:
        """ä»R2æ¢å¤æœ€æ–°çš„å¤‡ä»½ï¼Œå¦‚æœR2ä¸å¯ç”¨åˆ™ä½¿ç”¨æœ¬åœ°å¤‡ä»½"""
        try:
            logger.info("ğŸ”„ Starting R2 restore...")

            # é¦–å…ˆå°è¯•R2æ¢å¤
            try:
                return await self._restore_from_r2_cloud()
            except Exception as r2_error:
                logger.warning(f"âš ï¸ R2æ¢å¤å¤±è´¥: {r2_error}")
                logger.info("ğŸ”„ å°è¯•ä½¿ç”¨æœ¬åœ°å¤‡ä»½æ¢å¤...")

                # å›é€€åˆ°æœ¬åœ°å¤‡ä»½
                return await self._restore_from_local_backup()

        except Exception as e:
            logger.error(f"âŒ æ‰€æœ‰æ¢å¤æ–¹æ³•éƒ½å¤±è´¥: {e}")
            raise

    async def _restore_from_r2_cloud(self) -> Dict[str, Any]:
        """ä»R2äº‘å­˜å‚¨æ¢å¤"""
        # æ£€æŸ¥R2é…ç½®
        if not self._is_r2_configured():
            error_msg = "R2äº‘å­˜å‚¨æœªé…ç½®ã€‚è¯·æ£€æŸ¥ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼šR2_ACCESS_KEY_ID, R2_SECRET_ACCESS_KEY, R2_ENDPOINT, R2_BUCKET_NAME"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

        # éªŒè¯é…ç½®å€¼
        missing_configs = []
        if not self.r2_config.get('access_key'):
            missing_configs.append('R2_ACCESS_KEY_ID')
        if not self.r2_config.get('secret_key'):
            missing_configs.append('R2_SECRET_ACCESS_KEY')
        if not self.r2_config.get('endpoint'):
            missing_configs.append('R2_ENDPOINT')
        if not self.r2_config.get('bucket'):
            missing_configs.append('R2_BUCKET_NAME')

        if missing_configs:
            error_msg = f"R2é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘: {', '.join(missing_configs)}"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

        logger.info(f"âœ… R2é…ç½®éªŒè¯é€šè¿‡ - Bucket: {self.r2_config['bucket']}")

        # åˆ›å»ºS3å®¢æˆ·ç«¯ï¼Œé…ç½®ä¸ºR2
        try:
            from botocore.config import Config
            config = Config(
                region_name='auto',
                retries={'max_attempts': 3, 'mode': 'standard'},
                read_timeout=30,
                connect_timeout=15,
                signature_version='s3v4'
            )

            s3_client = boto3.client(
                's3',
                aws_access_key_id=self.r2_config['access_key'],
                aws_secret_access_key=self.r2_config['secret_key'],
                endpoint_url=self.r2_config['endpoint'],
                config=config
            )
            logger.info("âœ… S3å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            error_msg = f"åˆ›å»ºR2å®¢æˆ·ç«¯å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

        # åˆ—å‡ºR2ä¸­çš„å¤‡ä»½æ–‡ä»¶
        try:
            logger.info("ğŸ“‹ æ­£åœ¨åˆ—å‡ºR2ä¸­çš„å¤‡ä»½æ–‡ä»¶...")
            logger.info(f"ğŸ” æœç´¢å­˜å‚¨æ¡¶: {self.r2_config['bucket']}, å‰ç¼€: backups/")

            response = s3_client.list_objects_v2(
                Bucket=self.r2_config['bucket'],
                Prefix='backups/'
            )
            logger.info(f"âœ… æˆåŠŸè¿æ¥åˆ°R2å­˜å‚¨æ¡¶: {self.r2_config['bucket']}")
        except ClientError as e:
            error_code = e.response['Error']['Code']
            error_msg = e.response['Error']['Message']
            logger.error(f"âŒ æ— æ³•è®¿é—®R2å­˜å‚¨æ¡¶ (AWS Error {error_code}): {error_msg}")
            logger.error(f"ğŸ” è°ƒè¯•ä¿¡æ¯: å­˜å‚¨æ¡¶={self.r2_config['bucket']}, ç«¯ç‚¹={self.r2_config['endpoint']}")
            raise Exception(f"æ— æ³•è®¿é—®R2å­˜å‚¨æ¡¶: {error_msg}")
        except Exception as e:
            error_msg = f"è¿æ¥R2å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            logger.error(f"ğŸ” è°ƒè¯•ä¿¡æ¯: ç±»å‹={type(e).__name__}, æ¶ˆæ¯={str(e)}")
            raise Exception(error_msg)

        if 'Contents' not in response or not response['Contents']:
            error_msg = f"åœ¨R2å­˜å‚¨æ¡¶ '{self.r2_config['bucket']}' ä¸­æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶ã€‚è¯·ç¡®ä¿å·²ä¸Šä¼ å¤‡ä»½æ–‡ä»¶ã€‚"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

        # ä»…è€ƒè™‘ zip æ–‡ä»¶
        objects = [o for o in response['Contents'] if o.get('Key','').endswith('.zip')]
        if not objects:
            raise Exception("R2ä¸­æ²¡æœ‰å¯ç”¨çš„ .zip å¤‡ä»½æ–‡ä»¶")

        # ä¼˜å…ˆä½¿ç”¨ light è½»é‡å¤‡ä»½ï¼ˆæ–‡ä»¶ååŒ…å« _light_ï¼‰
        light_objs = [o for o in objects if '_light_' in o.get('Key','')]
        selected_pool = light_objs if light_objs else objects
        if light_objs:
            logger.info(f"ğŸª¶ æ£€æµ‹åˆ° {len(light_objs)} ä¸ª light å¤‡ä»½ï¼Œä¼˜å…ˆé€‰æ‹©å…¶ä¸­æœ€æ–°çš„è¿›è¡Œæ¢å¤")
        latest_backup = max(selected_pool, key=lambda x: x['LastModified'])
        backup_key = latest_backup['Key']
        backup_size = latest_backup['Size']
        backup_date = latest_backup['LastModified']

        logger.info(f"ğŸ“¥ æ‰¾åˆ°æœ€æ–°å¤‡ä»½: {backup_key}")
        logger.info(f"   ğŸ“… ä¿®æ”¹æ—¶é—´: {backup_date}")
        logger.info(f"   ğŸ“ æ–‡ä»¶å¤§å°: {backup_size} bytes")

        # ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
        try:
            self.backup_dir.mkdir(exist_ok=True)
            logger.info(f"âœ… å¤‡ä»½ç›®å½•å‡†å¤‡å°±ç»ª: {self.backup_dir}")
        except Exception as e:
            error_msg = f"åˆ›å»ºå¤‡ä»½ç›®å½•å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

        local_backup_path = self.backup_dir / Path(backup_key).name

        # ä¸‹è½½å¤‡ä»½æ–‡ä»¶
        try:
            logger.info(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½å¤‡ä»½æ–‡ä»¶åˆ°: {local_backup_path}")
            await asyncio.to_thread(
                s3_client.download_file,
                self.r2_config['bucket'],
                backup_key,
                str(local_backup_path)
            )
            logger.info("âœ… æ–‡ä»¶ä¸‹è½½å®Œæˆ")
        except ClientError as e:
            error_code = e.response['Error']['Code']
            error_msg = e.response['Error']['Message']
            logger.error(f"âŒ ä¸‹è½½å¤±è´¥ (AWS Error {error_code}): {error_msg}")
            raise Exception(f"ä»R2ä¸‹è½½å¤‡ä»½å¤±è´¥: {error_msg}")
        except Exception as e:
            error_msg = f"ä¸‹è½½å¤‡ä»½æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

        # éªŒè¯ä¸‹è½½çš„æ–‡ä»¶
        if not local_backup_path.exists():
            error_msg = f"ä¸‹è½½å®Œæˆåæ–‡ä»¶ä¸å­˜åœ¨: {local_backup_path}"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

        downloaded_size = local_backup_path.stat().st_size
        if downloaded_size != backup_size:
            error_msg = f"æ–‡ä»¶ä¸‹è½½ä¸å®Œæ•´ã€‚æœŸæœ›: {backup_size} bytes, å®é™…: {downloaded_size} bytes"
            logger.error(f"âŒ {error_msg}")
            # åˆ é™¤ä¸å®Œæ•´çš„æ–‡ä»¶
            local_backup_path.unlink(missing_ok=True)
            raise Exception(error_msg)

        logger.info(f"âœ… å¤‡ä»½æ–‡ä»¶éªŒè¯é€šè¿‡: {local_backup_path} ({downloaded_size} bytes)")

        # æ¢å¤å¤‡ä»½
        try:
            logger.info("ğŸ”„ æ­£åœ¨æ¢å¤å¤‡ä»½...")
            success = await self.restore_backup(local_backup_path.name)

            if success:
                restore_info = {
                    "filename": local_backup_path.name,
                    "size": downloaded_size,
                    "timestamp": datetime.now().isoformat(),
                    "source": "r2",
                    "r2_key": backup_key,
                    "r2_bucket": self.r2_config['bucket'],
                    "backup_date": backup_date.isoformat(),
                    "success": True
                }
                logger.info(f"âœ… R2æ¢å¤å®Œæˆ: {local_backup_path.name}")
                return restore_info
            else:
                error_msg = "å¤‡ä»½æ¢å¤è¿‡ç¨‹è¿”å›å¤±è´¥çŠ¶æ€"
                logger.error(f"âŒ {error_msg}")
                raise Exception(error_msg)

        except Exception as e:
            error_msg = f"æ¢å¤å¤‡ä»½æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            raise Exception(error_msg)

    async def _restore_from_local_backup(self) -> Dict[str, Any]:
        """ä»æœ¬åœ°å¤‡ä»½æ¢å¤"""
        try:
            logger.info("ğŸ”„ ä½¿ç”¨æœ¬åœ°å¤‡ä»½è¿›è¡Œæ¢å¤...")

            # æŸ¥æ‰¾å¯ç”¨çš„æœ¬åœ°å¤‡ä»½
            if not self.backup_dir.exists():
                raise Exception("å¤‡ä»½ç›®å½•ä¸å­˜åœ¨")

            backup_files = list(self.backup_dir.glob("*.zip"))
            if not backup_files:
                raise Exception("æ²¡æœ‰æ‰¾åˆ°æœ¬åœ°å¤‡ä»½æ–‡ä»¶")

            # ä½¿ç”¨æœ€æ–°çš„å¤‡ä»½
            latest_backup = max(backup_files, key=lambda x: x.stat().st_mtime)
            logger.info(f"ğŸ“ ä½¿ç”¨æœ¬åœ°å¤‡ä»½: {latest_backup.name}")

            # æ¢å¤å¤‡ä»½
            success = await self.restore_backup(latest_backup.name)

            if success:
                restore_info = {
                    "filename": latest_backup.name,
                    "size": latest_backup.stat().st_size,
                    "timestamp": datetime.now().isoformat(),
                    "source": "local",
                    "success": True
                }
                logger.info(f"âœ… æœ¬åœ°å¤‡ä»½æ¢å¤å®Œæˆ: {latest_backup.name}")
                return restore_info
            else:
                raise Exception("æœ¬åœ°å¤‡ä»½æ¢å¤å¤±è´¥")

        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°å¤‡ä»½æ¢å¤å¤±è´¥: {e}")
            raise


# åˆ›å»ºå…¨å±€å¤‡ä»½æœåŠ¡å®ä¾‹
backup_service = BackupService()


async def create_backup(backup_type: str = "full", upload_to_r2: bool = True) -> str:
    """åˆ›å»ºå¤‡ä»½"""
    return await backup_service.create_backup(backup_type, upload_to_r2)


async def list_backups() -> list:
    """åˆ—å‡ºå¤‡ä»½"""
    return backup_service.list_backups()

async def create_external_sql_backup() -> str:
    """Module-level helper: ä»…å¯¼å‡ºå¤–éƒ¨æ•°æ®åº“ SQL åˆ°æœ¬åœ°å¤‡ä»½ã€‚"""
    return await backup_service.create_external_sql_backup()


async def list_r2_files(prefix: Optional[str] = None) -> list:
    """Module-level helper: åˆ—å‡ºR2ä¸­æŒ‡å®šå‰ç¼€ä¸‹çš„å¯¹è±¡ï¼ˆè¿”å› key/name, size, last_modifiedï¼‰

    Args:
        prefix: ä»…åˆ—å‡ºè¯¥å‰ç¼€ä¸‹çš„å¯¹è±¡ï¼›é»˜è®¤åˆ—å‡º backups/
    """
    # use global backup_service instance
    if not backup_service._is_r2_configured():
        return []
    try:
        prefix = prefix or 'backups/'
        import boto3
        s3 = boto3.client(
            's3',
            endpoint_url=os.getenv('R2_ENDPOINT'),
            aws_access_key_id=os.getenv('R2_ACCESS_KEY_ID'),
            aws_secret_access_key=os.getenv('R2_SECRET_ACCESS_KEY'),
        )
        bucket = os.getenv('R2_BUCKET_NAME')
        if not bucket:
            return []
        paginator = s3.get_paginator('list_objects_v2')
        result = []
        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
            for obj in page.get('Contents', []) or []:
                result.append({
                    'key': obj.get('Key'),
                    'size': obj.get('Size'),
                    'last_modified': obj.get('LastModified').isoformat() if obj.get('LastModified') else None
                })
        return result
    except Exception as e:
        logger.warning(f"åˆ—å‡º R2 æ–‡ä»¶å¤±è´¥: {e}")
        return []

async def fetch_latest_r2_users_snapshot() -> Optional[list]:
    """ä¸‹è½½ R2 ä¸­æœ€æ–°çš„ï¼ˆä¼˜å…ˆ lightï¼‰å¤‡ä»½å¹¶è¿”å› users.json åˆ—è¡¨ã€‚

    è¿”å›ï¼š
        users åˆ—è¡¨(dict) æˆ– None ï¼ˆæœªæ‰¾åˆ° / å¤±è´¥ï¼‰
    ä»…ç”¨äºå¯åŠ¨è‡ªåŠ¨æ¢å¤ç”¨æˆ·ã€‚ä¸ä¼šæ‰§è¡Œå…¨é‡æ¢å¤ï¼Œç®€å•è§£æ users.jsonã€‚
    """
    if not backup_service._is_r2_configured():
        return None
    try:
        files = await list_r2_files('backups/')
        if not files:
            return None
        # è¿‡æ»¤å‡º zip
        zips = [f for f in files if f.get('key','').endswith('.zip')]
        if not zips:
            return None
        # light ä¼˜å…ˆï¼šæŒ‰æ˜¯å¦åŒ…å« 'light' æ ‡è®°ä¼˜å…ˆæ’åºï¼Œå†æŒ‰æ—¶é—´
        def rank(f):
            key = f.get('key','').lower()
            is_light = 'light' in key
            ts = f.get('last_modified') or ''
            return (0 if is_light else 1, 0 - len(ts), ts)
        # å…ˆæŒ‰ last_modified å·²æ˜¯é™åºï¼Œè¿™é‡Œç›´æ¥éå†æŒ‘ç¬¬ä¸€ä¸ª lightï¼Œå¦åˆ™ç¬¬ä¸€ä¸ª
        candidate = None
        for f in zips:
            if 'light' in f.get('key','').lower():
                candidate = f; break
        if candidate is None:
            candidate = zips[0]
        from botocore.config import Config
        config = Config(region_name='auto')
        s3_client = boto3.client(
            's3',
            aws_access_key_id=backup_service.r2_config['access_key'],
            aws_secret_access_key=backup_service.r2_config['secret_key'],
            endpoint_url=backup_service.r2_config['endpoint'],
            config=config
        )
        import tempfile, zipfile, json
        with tempfile.TemporaryDirectory() as td:
            local_zip = os.path.join(td, 'backup.zip')
            await asyncio.to_thread(s3_client.download_file, backup_service.r2_config['bucket'], candidate['key'], local_zip)
            with zipfile.ZipFile(local_zip,'r') as zf:
                # users.json å¯èƒ½åœ¨ data/ ç›®å½•
                for name in zf.namelist():
                    if name.endswith('data/users.json') or name.endswith('/users.json') or name == 'users.json':
                        try:
                            with zf.open(name) as f:
                                data = f.read().decode('utf-8')
                                return json.loads(data)
                        except Exception as ue:
                            logger.warning(f"è§£æ users.json å¤±è´¥: {ue}")
                            return None
        return None
    except Exception as e:
        logger.warning(f"fetch_latest_r2_users_snapshot failed: {e}")
        return None

    try:
        from botocore.config import Config
        config = Config(region_name='auto')
        s3_client = boto3.client(
            's3',
            aws_access_key_id=backup_service.r2_config['access_key'],
            aws_secret_access_key=backup_service.r2_config['secret_key'],
            endpoint_url=backup_service.r2_config['endpoint'],
            config=config
        )
        effective_prefix = prefix or 'backups/'
        response = s3_client.list_objects_v2(
            Bucket=backup_service.r2_config['bucket'],
            Prefix=effective_prefix
        )
        items = []
        if 'Contents' in response and response['Contents']:
            for obj in response['Contents']:
                items.append({
                    'key': obj.get('Key'),
                    'size': obj.get('Size'),
                    'last_modified': obj.get('LastModified').isoformat() if obj.get('LastModified') else None
                })
        items.sort(key=lambda x: x.get('last_modified') or '', reverse=True)
        return items
    except Exception as e:
        logger.warning(f"list_r2_files failed: {e}")
        return []


async def delete_r2_file(key: str) -> bool:
    """Module-level helper: ä»R2åˆ é™¤æŒ‡å®šå¯¹è±¡ï¼ˆkeyï¼‰"""
    if not backup_service._is_r2_configured():
        raise Exception("R2æœªé…ç½®")

    try:
        from botocore.config import Config
        config = Config(region_name='auto')
        s3_client = boto3.client(
            's3',
            aws_access_key_id=backup_service.r2_config['access_key'],
            aws_secret_access_key=backup_service.r2_config['secret_key'],
            endpoint_url=backup_service.r2_config['endpoint'],
            config=config
        )

        await asyncio.to_thread(s3_client.delete_object, Bucket=backup_service.r2_config['bucket'], Key=key)
        logger.info(f"âœ… R2 object deleted: {key}")
        return True
    except Exception as e:
        logger.error(f"âŒ delete_r2_file failed: {e}")
        return False


async def restore_r2_key(key: str) -> Dict[str, Any]:
    """Module-level helper: ä»R2ä¸‹è½½æŒ‡å®š key å¹¶æ¢å¤è¯¥å¤‡ä»½ï¼ˆå°†æ–‡ä»¶ä¸‹è½½åˆ° backup_dir å¹¶è°ƒç”¨ restore_backupï¼‰"""
    if not backup_service._is_r2_configured():
        raise Exception("R2æœªé…ç½®")

    try:
        from botocore.config import Config
        config = Config(region_name='auto')
        s3_client = boto3.client(
            's3',
            aws_access_key_id=backup_service.r2_config['access_key'],
            aws_secret_access_key=backup_service.r2_config['secret_key'],
            endpoint_url=backup_service.r2_config['endpoint'],
            config=config
        )

        backup_service.backup_dir.mkdir(exist_ok=True)
        local_backup_path = backup_service.backup_dir / Path(key).name

        await asyncio.to_thread(
            s3_client.download_file,
            backup_service.r2_config['bucket'],
            key,
            str(local_backup_path)
        )

        if not local_backup_path.exists():
            raise Exception("ä¸‹è½½åçš„æ–‡ä»¶ä¸å­˜åœ¨")

        success = await backup_service.restore_backup(local_backup_path.name)
        if not success:
            raise Exception("æ¢å¤å¤±è´¥")

        return {
            'filename': local_backup_path.name,
            'size': local_backup_path.stat().st_size,
            'timestamp': datetime.now().isoformat(),
            'source': 'r2',
            'r2_key': key,
            'r2_bucket': backup_service.r2_config['bucket'],
            'success': True
        }
    except Exception as e:
        logger.error(f"âŒ restore_r2_key failed: {e}")
        raise


async def restore_backup(backup_name: str) -> bool:
    """æ¢å¤å¤‡ä»½"""
    return await backup_service.restore_backup(backup_name)
