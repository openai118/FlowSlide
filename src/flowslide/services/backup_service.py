"""
å¤‡ä»½æœåŠ¡ - é›†æˆR2äº‘å¤‡ä»½å’Œæœ¬åœ°å¤‡ä»½åŠŸèƒ½
"""

import asyncio
import logging
import os
import shutil
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

import boto3
from botocore.exceptions import ClientError
from dotenv import load_dotenv

logger = logging.getLogger(__name__)


class BackupService:
    """å¤‡ä»½æœåŠ¡"""

    def __init__(self):
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()
        
        self.backup_dir = Path("./backups")
        self.backup_dir.mkdir(exist_ok=True)

        # R2é…ç½®
        self.r2_config = {
            "access_key": os.getenv("R2_ACCESS_KEY_ID"),
            "secret_key": os.getenv("R2_SECRET_ACCESS_KEY"),
            "endpoint": os.getenv("R2_ENDPOINT"),
            "bucket": os.getenv("R2_BUCKET_NAME")
        }

        # å¤‡ä»½é…ç½®
        self.retention_days = int(os.getenv("BACKUP_RETENTION_DAYS", "30"))
        self.webhook_url = os.getenv("BACKUP_WEBHOOK_URL")

    async def create_backup(self, backup_type: str = "full") -> str:
        """åˆ›å»ºå¤‡ä»½

        Args:
            backup_type: å¤‡ä»½ç±»å‹ (full, db_only, config_only)

        Returns:
            å¤‡ä»½æ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"flowslide_backup_{backup_type}_{timestamp}"
        backup_path = self.backup_dir / backup_name
        backup_path.mkdir(exist_ok=True)

        try:
            logger.info(f"ğŸ“¦ Creating {backup_type} backup: {backup_name}")

            if backup_type in ["full", "db_only"]:
                await self._backup_database(backup_path)

            if backup_type in ["full", "config_only"]:
                await self._backup_config(backup_path)

            if backup_type == "full":
                await self._backup_uploads(backup_path)

            # å‹ç¼©å¤‡ä»½
            archive_path = await self._compress_backup(backup_path)

            # ä¸Šä¼ åˆ°R2ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if self._is_r2_configured():
                await self._upload_to_r2(archive_path)

            # æ¸…ç†æ—§å¤‡ä»½
            await self._cleanup_old_backups()

            # å‘é€é€šçŸ¥
            if self.webhook_url:
                await self._send_notification(backup_name, "success")

            logger.info(f"âœ… Backup completed: {archive_path}")
            return str(archive_path)

        except Exception as e:
            logger.error(f"âŒ Backup failed: {e}")
            if self.webhook_url:
                await self._send_notification(backup_name, "failed", str(e))
            raise

    async def _backup_database(self, backup_path: Path):
        """å¤‡ä»½æ•°æ®åº“"""
        try:
            from ..database import db_manager

            if db_manager.database_type == "sqlite":
                # SQLiteæ•°æ®åº“å¤‡ä»½
                db_file = Path("./data/flowslide.db")
                if db_file.exists():
                    shutil.copy2(db_file, backup_path / "flowslide.db")
                    logger.info("ğŸ’¾ Database backup completed")
            else:
                # å¤–éƒ¨æ•°æ®åº“å¤‡ä»½
                await self._backup_external_database(backup_path)

        except Exception as e:
            logger.error(f"âŒ Database backup failed: {e}")
            raise

    async def _backup_external_database(self, backup_path: Path):
        """å¤‡ä»½å¤–éƒ¨æ•°æ®åº“"""
        # è¿™é‡Œå¯ä»¥å®ç°å¤–éƒ¨æ•°æ®åº“çš„å¤‡ä»½é€»è¾‘
        # ä¾‹å¦‚ä½¿ç”¨pg_dump for PostgreSQL
        logger.info("ğŸ’¾ External database backup (not implemented)")

    async def _backup_config(self, backup_path: Path):
        """å¤‡ä»½é…ç½®æ–‡ä»¶"""
        config_files = [".env", "pyproject.toml", "uv.toml"]

        for config_file in config_files:
            if Path(config_file).exists():
                shutil.copy2(config_file, backup_path / config_file)

        logger.info("âš™ï¸ Config backup completed")

    async def _backup_uploads(self, backup_path: Path):
        """å¤‡ä»½ä¸Šä¼ æ–‡ä»¶"""
        uploads_dir = Path("./uploads")
        if uploads_dir.exists():
            shutil.copytree(uploads_dir, backup_path / "uploads", dirs_exist_ok=True)
            logger.info("ğŸ“ Uploads backup completed")

    async def _compress_backup(self, backup_path: Path) -> Path:
        """å‹ç¼©å¤‡ä»½æ–‡ä»¶"""
        import zipfile

        archive_path = backup_path.with_suffix('.zip')

        with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_path in backup_path.rglob('*'):
                if file_path.is_file():
                    zipf.write(file_path, file_path.relative_to(backup_path.parent))

        # åˆ é™¤åŸå§‹å¤‡ä»½ç›®å½•
        shutil.rmtree(backup_path)

        logger.info(f"ğŸ—œï¸ Backup compressed: {archive_path}")
        return archive_path

    async def _upload_to_r2(self, backup_path: Path):
        """ä¸Šä¼ å¤‡ä»½åˆ°R2"""
        if not self._is_r2_configured():
            logger.info("â„¹ï¸ R2 not configured, skipping cloud backup")
            return

        try:
            logger.info(f"â˜ï¸ Starting R2 upload: {backup_path.name}")

            # åˆ›å»ºS3å®¢æˆ·ç«¯ï¼Œé…ç½®ä¸ºR2
            s3_client = boto3.client(
                's3',
                aws_access_key_id=self.r2_config['access_key'],
                aws_secret_access_key=self.r2_config['secret_key'],
                endpoint_url=self.r2_config['endpoint'],
                region_name='auto'  # R2ä½¿ç”¨auto region
            )

            # ç”Ÿæˆå¤‡ä»½æ—¥æœŸç›®å½•
            backup_date = datetime.now().strftime("%Y-%m-%d")
            s3_key = f"backups/{backup_date}/{backup_path.name}"

            # ä¸Šä¼ æ–‡ä»¶
            logger.info(f"Uploading to R2: {self.r2_config['bucket']}/{s3_key}")

            # ä½¿ç”¨asyncio.to_threadåœ¨åå°çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥çš„boto3æ“ä½œ
            await asyncio.to_thread(
                s3_client.upload_file,
                str(backup_path),
                self.r2_config['bucket'],
                s3_key
            )

            logger.info(f"âœ… R2 upload completed successfully: {backup_path.name}")

        except ClientError as e:
            error_code = e.response['Error']['Code']
            error_msg = e.response['Error']['Message']
            logger.error(f"âŒ R2 upload failed (AWS Error {error_code}): {error_msg}")
            raise Exception(f"R2 upload failed: {error_msg}")
        except Exception as e:
            logger.error(f"âŒ R2 upload failed: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºæœ¬åœ°å¤‡ä»½å·²ç»æˆåŠŸ

    def _is_r2_configured(self) -> bool:
        """æ£€æŸ¥R2æ˜¯å¦é…ç½®"""
        return all(self.r2_config.values())

    async def _cleanup_old_backups(self):
        """æ¸…ç†æ—§å¤‡ä»½"""
        try:
            cutoff_date = datetime.now().timestamp() - (self.retention_days * 24 * 60 * 60)

            for backup_file in self.backup_dir.glob("*.zip"):
                if backup_file.stat().st_mtime < cutoff_date:
                    backup_file.unlink()
                    logger.info(f"ğŸ—‘ï¸ Cleaned up old backup: {backup_file.name}")

        except Exception as e:
            logger.error(f"âŒ Cleanup failed: {e}")

    async def _send_notification(self, backup_name: str, status: str, error: Optional[str] = None):
        """å‘é€å¤‡ä»½é€šçŸ¥"""
        if not self.webhook_url:
            return

        try:
            import aiohttp

            message = {
                "backup_name": backup_name,
                "status": status,
                "timestamp": datetime.now().isoformat(),
                "error": error
            }

            async with aiohttp.ClientSession() as session:
                await session.post(self.webhook_url, json=message)

        except Exception as e:
            logger.error(f"âŒ Notification failed: {e}")

    async def sync_to_r2(self, backup_path: Optional[Path] = None) -> Dict[str, Any]:
        """åŒæ­¥å¤‡ä»½åˆ°R2äº‘å­˜å‚¨

        Args:
            backup_path: æŒ‡å®šçš„å¤‡ä»½æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ€æ–°çš„å¤‡ä»½

        Returns:
            åŒæ­¥ç»“æœä¿¡æ¯
        """
        if not self._is_r2_configured():
            raise Exception("R2äº‘å­˜å‚¨æœªé…ç½®")

        try:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šå¤‡ä»½è·¯å¾„ï¼Œä½¿ç”¨æœ€æ–°çš„å¤‡ä»½
            if backup_path is None:
                backups = await list_backups()
                if not backups:
                    raise Exception("æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶è¿›è¡ŒåŒæ­¥")
                backup_path = Path(backups[0]["path"])

            # ç¡®ä¿å¤‡ä»½æ–‡ä»¶å­˜åœ¨
            if not backup_path.exists():
                raise FileNotFoundError(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_path}")

            logger.info(f"â˜ï¸ Starting R2 sync: {backup_path.name}")

            # ä¸Šä¼ åˆ°R2
            await self._upload_to_r2(backup_path)

            sync_info = {
                "filename": backup_path.name,
                "size": backup_path.stat().st_size,
                "timestamp": datetime.now().isoformat(),
                "success": True
            }

            logger.info(f"âœ… R2 sync completed: {backup_path.name}")
            return sync_info

        except Exception as e:
            logger.error(f"âŒ R2 sync failed: {e}")
            raise

    def list_backups(self) -> list:
        """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½"""
        backups = []
        for backup_file in self.backup_dir.glob("*.zip"):
            stat = backup_file.stat()
            backups.append({
                "name": backup_file.name,
                "size": stat.st_size,
                "created": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                "path": str(backup_file)
            })

        return sorted(backups, key=lambda x: x["created"], reverse=True)

    async def restore_backup(self, backup_name: str) -> bool:
        """æ¢å¤å¤‡ä»½"""
        backup_path = self.backup_dir / backup_name
        if not backup_path.exists():
            raise FileNotFoundError(f"Backup not found: {backup_name}")

        try:
            logger.info(f"ğŸ”„ Restoring backup: {backup_name}")

            # åˆ›å»ºä¸´æ—¶æ¢å¤ç›®å½•
            restore_temp_dir = self.backup_dir / f"restore_temp_{int(time.time())}"
            restore_temp_dir.mkdir(exist_ok=True)

            def extract_and_restore():
                import zipfile
                import shutil
                from pathlib import Path

                try:
                    # è§£å‹å¤‡ä»½æ–‡ä»¶
                    logger.info(f"ğŸ“¦ Extracting backup: {backup_name}")
                    with zipfile.ZipFile(str(backup_path), 'r') as zip_ref:
                        zip_ref.extractall(str(restore_temp_dir))

                    # æŸ¥æ‰¾æ•°æ®åº“æ–‡ä»¶
                    db_files = list(restore_temp_dir.glob("*.db"))
                    if not db_files:
                        raise Exception("å¤‡ä»½æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æ•°æ®åº“æ–‡ä»¶")

                    db_file = db_files[0]
                    logger.info(f"ğŸ—„ï¸ Found database file: {db_file.name}")

                    # å¤‡ä»½å½“å‰æ•°æ®åº“
                    current_db_path = Path("./data/flowslide.db")
                    if current_db_path.exists():
                        backup_current = current_db_path.with_suffix('.db.backup')
                        shutil.copy2(str(current_db_path), str(backup_current))
                        logger.info(f"ğŸ’¾ Backed up current database to: {backup_current}")

                    # æ¢å¤æ•°æ®åº“æ–‡ä»¶
                    shutil.copy2(str(db_file), str(current_db_path))
                    logger.info(f"âœ… Database restored from: {db_file.name}")

                    # æ¢å¤ä¸Šä¼ æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    uploads_dir = restore_temp_dir / "uploads"
                    if uploads_dir.exists():
                        target_uploads = Path("./uploads")
                        if target_uploads.exists():
                            shutil.rmtree(str(target_uploads))
                        shutil.copytree(str(uploads_dir), str(target_uploads))
                        logger.info("ğŸ“ Uploads directory restored")

                    # æ¢å¤é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    config_files = list(restore_temp_dir.glob("*.json")) + list(restore_temp_dir.glob("*.yaml")) + list(restore_temp_dir.glob("*.yml"))
                    for config_file in config_files:
                        if "flowslide" in config_file.name.lower():
                            target_config = Path(".") / config_file.name
                            shutil.copy2(str(config_file), str(target_config))
                            logger.info(f"âš™ï¸ Config file restored: {config_file.name}")

                    logger.info("âœ… Backup restored successfully")
                    return True

                except Exception as e:
                    logger.error(f"âŒ Restore operation failed: {e}")
                    # å°è¯•æ¢å¤åŸå§‹æ•°æ®åº“
                    current_db_path = Path("./data/flowslide.db")
                    backup_current = current_db_path.with_suffix('.db.backup')
                    if backup_current.exists():
                        shutil.copy2(str(backup_current), str(current_db_path))
                        logger.info("ğŸ”„ Original database restored from backup")
                    raise
                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if restore_temp_dir.exists():
                        shutil.rmtree(str(restore_temp_dir))
                        logger.info("ğŸ§¹ Temporary restore files cleaned up")

            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œæ¢å¤æ“ä½œ
            await asyncio.to_thread(extract_and_restore)
            return True

        except Exception as e:
            logger.error(f"âŒ Restore failed: {e}")
            return False

    async def restore_from_r2(self) -> Dict[str, Any]:
        """ä»R2æ¢å¤æœ€æ–°çš„å¤‡ä»½"""
        if not self._is_r2_configured():
            raise Exception("R2äº‘å­˜å‚¨æœªé…ç½®")

        try:
            logger.info("ğŸ”„ Starting R2 restore...")

            # åˆ›å»ºS3å®¢æˆ·ç«¯ï¼Œé…ç½®ä¸ºR2
            s3_client = boto3.client(
                's3',
                aws_access_key_id=self.r2_config['access_key'],
                aws_secret_access_key=self.r2_config['secret_key'],
                endpoint_url=self.r2_config['endpoint'],
                region_name='auto'  # R2ä½¿ç”¨auto region
            )

            # åˆ—å‡ºR2ä¸­çš„å¤‡ä»½æ–‡ä»¶
            response = s3_client.list_objects_v2(
                Bucket=self.r2_config['bucket'],
                Prefix='backups/'
            )

            if 'Contents' not in response or not response['Contents']:
                raise Exception("R2ä¸­æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶")

            # æ‰¾åˆ°æœ€æ–°çš„å¤‡ä»½æ–‡ä»¶
            latest_backup = max(response['Contents'], key=lambda x: x['LastModified'])
            backup_key = latest_backup['Key']
            backup_size = latest_backup['Size']
            local_backup_path = self.backup_dir / Path(backup_key).name

            logger.info(f"ğŸ“¥ Downloading latest backup from R2: {backup_key} (Size: {backup_size} bytes)")

            # ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
            self.backup_dir.mkdir(exist_ok=True)

            # ä¸‹è½½å¤‡ä»½æ–‡ä»¶
            await asyncio.to_thread(
                s3_client.download_file,
                self.r2_config['bucket'],
                backup_key,
                str(local_backup_path)
            )

            # éªŒè¯ä¸‹è½½çš„æ–‡ä»¶
            if not local_backup_path.exists():
                raise Exception("ä¸‹è½½çš„æ–‡ä»¶ä¸å­˜åœ¨")

            downloaded_size = local_backup_path.stat().st_size
            if downloaded_size != backup_size:
                raise Exception(f"æ–‡ä»¶ä¸‹è½½ä¸å®Œæ•´ã€‚æœŸæœ›: {backup_size} bytes, å®é™…: {downloaded_size} bytes")

            logger.info(f"âœ… Backup downloaded successfully: {local_backup_path} ({downloaded_size} bytes)")

            # æ¢å¤å¤‡ä»½
            success = await self.restore_backup(local_backup_path.name)

            if success:
                restore_info = {
                    "filename": local_backup_path.name,
                    "size": downloaded_size,
                    "timestamp": datetime.now().isoformat(),
                    "source": "r2",
                    "r2_key": backup_key,
                    "success": True
                }
                logger.info(f"âœ… R2 restore completed: {local_backup_path.name}")
                return restore_info
            else:
                raise Exception("å¤‡ä»½æ¢å¤å¤±è´¥")

        except ClientError as e:
            error_code = e.response['Error']['Code']
            error_msg = e.response['Error']['Message']
            logger.error(f"âŒ R2 restore failed (AWS Error {error_code}): {error_msg}")
            raise Exception(f"R2æ¢å¤å¤±è´¥: {error_msg}")
        except Exception as e:
            logger.error(f"âŒ R2 restore failed: {e}")
            raise


# åˆ›å»ºå…¨å±€å¤‡ä»½æœåŠ¡å®ä¾‹
backup_service = BackupService()


async def create_backup(backup_type: str = "full") -> str:
    """åˆ›å»ºå¤‡ä»½"""
    return await backup_service.create_backup(backup_type)


async def list_backups() -> list:
    """åˆ—å‡ºå¤‡ä»½"""
    return backup_service.list_backups()


async def restore_backup(backup_name: str) -> bool:
    """æ¢å¤å¤‡ä»½"""
    return await backup_service.restore_backup(backup_name)
